# ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • (Monocular Depth Estimation)

## ê°œìš”

ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì€ ë‹¨ì¼ 2D ì´ë¯¸ì§€ì—ì„œ í”½ì…€ë³„ ê¹Šì´ ì •ë³´ë¥¼ ì¶”ì •í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. MiDaS, DPT ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ê³¼ Structure from Motion (SfM)ì„ í†µí•œ ê¸°í•˜í•™ì  ì ‘ê·¼ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

**ë‚œì´ë„**: â­â­â­â­

**ì„ ìˆ˜ ì§€ì‹**: DNN ëª¨ë“ˆ, íŠ¹ì§•ì  ê²€ì¶œ/ë§¤ì¹­, ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜

---

## ëª©ì°¨

1. [ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • ê°œìš”](#1-ë‹¨ì•ˆ-ê¹Šì´-ì¶”ì •-ê°œìš”)
2. [MiDaS ëª¨ë¸](#2-midas-ëª¨ë¸)
3. [DPT (Dense Prediction Transformer)](#3-dpt-dense-prediction-transformer)
4. [Structure from Motion (SfM)](#4-structure-from-motion-sfm)
5. [ê¹Šì´ ë§µ ì‘ìš©](#5-ê¹Šì´-ë§µ-ì‘ìš©)
6. [ì—°ìŠµ ë¬¸ì œ](#6-ì—°ìŠµ-ë¬¸ì œ)

---

## 1. ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • ê°œìš”

### ì™œ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì¸ê°€?

```
ìŠ¤í…Œë ˆì˜¤ vs ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ìŠ¤í…Œë ˆì˜¤ ë¹„ì „                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚   ğŸ“·      â”‚    â”‚     ğŸ“·    â”‚                                 â”‚
â”‚  â”‚   Left    â”‚â—„â”€â”€â–ºâ”‚   Right   â”‚  ë‘ ì¹´ë©”ë¼ í•„ìš”                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                                                                 â”‚
â”‚  ì¥ì : ê¸°í•˜í•™ì ìœ¼ë¡œ ì •í™•, ì ˆëŒ€ ê¹Šì´ ì¸¡ì • ê°€ëŠ¥                   â”‚
â”‚  ë‹¨ì : ë‘ ì¹´ë©”ë¼ í•„ìš”, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìˆ˜                        â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚
â”‚  â”‚    ğŸ“·     â”‚  ë‹¨ì¼ ì¹´ë©”ë¼ë¡œ ê°€ëŠ¥                              â”‚
â”‚  â”‚  Single   â”‚  ìŠ¤ë§ˆíŠ¸í°, ë“œë¡ , ë¡œë´‡ ë“±ì— ì í•©                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
â”‚                                                                 â”‚
â”‚  ì¥ì : ë‹¨ì¼ ì¹´ë©”ë¼, ê°„ë‹¨í•œ ì„¤ì •, ì´ë™ ì¥ì¹˜ì— ì í•©               â”‚
â”‚  ë‹¨ì : ìƒëŒ€ì  ê¹Šì´, ìŠ¤ì¼€ì¼ ëª¨í˜¸ì„±, í•™ìŠµ ë°ì´í„° ì˜ì¡´             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê¹Šì´ ì¶”ì •ì˜ ì–´ë ¤ì›€

```
ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì˜ ë³¸ì§ˆì  ëª¨í˜¸ì„±:

ë™ì¼í•œ 2D ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë¬´í•œíˆ ë§ì€ 3D ì¥ë©´ì´ ì¡´ì¬

                        â”‚
                        â”‚
         â—              â”‚         ğŸ¾  ì‘ì€ ê³µ, ê°€ê¹Œì´
        /â”‚\             â”‚
         â”‚              â”‚
                        â”‚
                        â”‚         ğŸ€  í° ê³µ, ë©€ë¦¬
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[ğŸ“·]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ê°™ì€ í¬ê¸°ë¡œ ë³´ì„!

í•´ê²° ë°©ë²•:
1. í•™ìŠµëœ ì‚¬ì „ ì§€ì‹ (ë”¥ëŸ¬ë‹)
   - ë¬¼ì²´ì˜ ì¼ë°˜ì ì¸ í¬ê¸°
   - ì›ê·¼ê° ê·œì¹™
   - í…ìŠ¤ì²˜ ê·¸ë˜ë””ì–¸íŠ¸

2. ë‹¤ì¤‘ ì´ë¯¸ì§€ (SfM)
   - ì‹œì  ë³€í™”ë¥¼ ì´ìš©
   - ê¸°í•˜í•™ì  ì œì•½

3. ì¶”ê°€ ì„¼ì„œ
   - LiDAR ë³´ì¡°
   - êµ¬ì¡°ê´‘ ë³´ì¡°
```

### ê¹Šì´ ì¶”ì • ë°©ë²•ë¡ 

```
ê¹Šì´ ì¶”ì • ì ‘ê·¼ë²•:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ì§€ë„ í•™ìŠµ (Supervised Learning)                              â”‚
â”‚    - RGB-D ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ                                    â”‚
â”‚    - Ground Truth ê¹Šì´ í•„ìš”                                     â”‚
â”‚    - ë°ì´í„°ì…‹: NYU Depth V2, KITTI, ScanNet                    â”‚
â”‚                                                                 â”‚
â”‚ 2. ìê¸°ì§€ë„ í•™ìŠµ (Self-supervised Learning)                     â”‚
â”‚    - ìŠ¤í…Œë ˆì˜¤ ìŒ ë˜ëŠ” ì—°ì† í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµ                      â”‚
â”‚    - Ground Truth ë¶ˆí•„ìš”                                        â”‚
â”‚    - Monodepth2, PackNet-SfM                                   â”‚
â”‚                                                                 â”‚
â”‚ 3. ì œë¡œìƒ· í•™ìŠµ (Zero-shot / Cross-domain)                       â”‚
â”‚    - ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ í•™ìŠµ                              â”‚
â”‚    - ìƒˆë¡œìš´ ë„ë©”ì¸ì— ì¼ë°˜í™”                                     â”‚
â”‚    - MiDaS, DPT, ZoeDepth                                      â”‚
â”‚                                                                 â”‚
â”‚ 4. ê¸°í•˜í•™ì  ë°©ë²• (Geometric Methods)                            â”‚
â”‚    - Structure from Motion                                      â”‚
â”‚    - Multi-View Stereo                                          â”‚
â”‚    - ëª…ì‹œì  ê¸°í•˜í•™ì  ì œì•½ ì‚¬ìš©                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. MiDaS ëª¨ë¸

### MiDaS ê°œìš”

```
MiDaS (Mixing Datasets for Monocular Depth Estimation):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  í•µì‹¬ ì•„ì´ë””ì–´: ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í˜¼í•©í•˜ì—¬ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ     â”‚
â”‚                                                                 â”‚
â”‚  í•™ìŠµ ë°ì´í„°:                                                   â”‚
â”‚  - ReDWeb (ì¸í„°ë„· ì´ë¯¸ì§€)                                       â”‚
â”‚  - DIML (ì‹¤ë‚´)                                                  â”‚
â”‚  - Movies (ì˜í™” ì¥ë©´)                                           â”‚
â”‚  - MegaDepth (ì•¼ì™¸)                                             â”‚
â”‚  - WSVD (ë¹„ë””ì˜¤)                                                â”‚
â”‚                                                                 â”‚
â”‚  íŠ¹ì§•:                                                          â”‚
â”‚  - ìŠ¤ì¼€ì¼ ë¶ˆë³€ (scale-invariant) ì†ì‹¤ í•¨ìˆ˜                      â”‚
â”‚  - ìƒëŒ€ì  ê¹Šì´ ì˜ˆì¸¡                                             â”‚
â”‚  - ë‹¤ì–‘í•œ ë°±ë³¸ (EfficientNet, ResNeXt, ViT)                    â”‚
â”‚                                                                 â”‚
â”‚  ëª¨ë¸ ë²„ì „:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ ëª¨ë¸             â”‚ ì…ë ¥ í¬ê¸° â”‚ íŠ¹ì§•                    â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ MiDaS v2.1 Large â”‚ 384x384   â”‚ ê³ í’ˆì§ˆ, ëŠë¦¼            â”‚     â”‚
â”‚  â”‚ MiDaS v2.1 Small â”‚ 256x256   â”‚ ê²½ëŸ‰, ë¹ ë¦„              â”‚     â”‚
â”‚  â”‚ MiDaS v3 (DPT)   â”‚ 384x384   â”‚ Transformer ê¸°ë°˜        â”‚     â”‚
â”‚  â”‚ MiDaS v3.1 (DPT) â”‚ ë‹¤ì–‘      â”‚ ìµœì‹ , ë‹¤ì–‘í•œ ë°±ë³¸       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MiDaS ì‚¬ìš©í•˜ê¸°

```python
import cv2
import numpy as np
import torch

def load_midas_model(model_type='DPT_Large'):
    """MiDaS ëª¨ë¸ ë¡œë“œ (PyTorch Hub)"""

    # ëª¨ë¸ íƒ€ì…:
    # - 'DPT_Large': ê°€ì¥ ì •í™•
    # - 'DPT_Hybrid': ê· í˜•
    # - 'MiDaS_small': ê°€ì¥ ë¹ ë¦„

    model = torch.hub.load('intel-isl/MiDaS', model_type)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    # ì „ì²˜ë¦¬ íŠ¸ëœìŠ¤í¼
    midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')

    if model_type in ['DPT_Large', 'DPT_Hybrid']:
        transform = midas_transforms.dpt_transform
    else:
        transform = midas_transforms.small_transform

    return model, transform, device

def estimate_depth_midas(img, model, transform, device):
    """MiDaSë¡œ ê¹Šì´ ì¶”ì •"""

    # BGR â†’ RGB
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # ì „ì²˜ë¦¬
    input_batch = transform(img_rgb).to(device)

    # ì¶”ë¡ 
    with torch.no_grad():
        prediction = model(input_batch)

        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img.shape[:2],
            mode='bicubic',
            align_corners=False
        ).squeeze()

    depth_map = prediction.cpu().numpy()

    return depth_map

def normalize_depth(depth_map):
    """ê¹Šì´ ë§µ ì •ê·œí™” (ì‹œê°í™”ìš©)"""

    depth_min = depth_map.min()
    depth_max = depth_map.max()

    depth_normalized = (depth_map - depth_min) / (depth_max - depth_min)
    depth_normalized = (depth_normalized * 255).astype(np.uint8)

    return depth_normalized

def colorize_depth(depth_map, colormap=cv2.COLORMAP_INFERNO):
    """ê¹Šì´ ë§µì— ì»¬ëŸ¬ë§µ ì ìš©"""

    depth_norm = normalize_depth(depth_map)
    depth_colored = cv2.applyColorMap(depth_norm, colormap)

    return depth_colored

# ì‚¬ìš© ì˜ˆ
def main():
    # ëª¨ë¸ ë¡œë“œ
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    model, transform, device = load_midas_model('DPT_Large')

    # ì´ë¯¸ì§€ ë¡œë“œ
    img = cv2.imread('sample.jpg')

    # ê¹Šì´ ì¶”ì •
    print("ê¹Šì´ ì¶”ì • ì¤‘...")
    depth = estimate_depth_midas(img, model, transform, device)

    # ì‹œê°í™”
    depth_colored = colorize_depth(depth)

    cv2.imshow('Original', img)
    cv2.imshow('Depth', depth_colored)
    cv2.waitKey(0)
```

### OpenCV DNNìœ¼ë¡œ MiDaS ì‹¤í–‰

```python
import cv2
import numpy as np

class MiDaSDepthEstimator:
    """OpenCV DNNìœ¼ë¡œ MiDaS ì‹¤í–‰"""

    def __init__(self, model_path):
        """
        model_path: ONNX ëª¨ë¸ ê²½ë¡œ
        ë‹¤ìš´ë¡œë“œ: https://github.com/isl-org/MiDaS/releases
        """
        self.net = cv2.dnn.readNetFromONNX(model_path)

        # GPU ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

        # ì…ë ¥ í¬ê¸° (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
        self.input_size = (384, 384)  # DPT_Large
        # self.input_size = (256, 256)  # MiDaS_small

    def estimate(self, img):
        """ê¹Šì´ ì¶”ì •"""

        h, w = img.shape[:2]

        # ì „ì²˜ë¦¬
        blob = cv2.dnn.blobFromImage(
            img,
            scalefactor=1/255.0,
            size=self.input_size,
            mean=(0.485, 0.456, 0.406),  # ImageNet mean
            swapRB=True,
            crop=False
        )

        # í‘œì¤€í¸ì°¨ ì •ê·œí™” (ìˆ˜ë™)
        std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)
        blob = blob / std

        # ì¶”ë¡ 
        self.net.setInput(blob)
        output = self.net.forward()

        # í›„ì²˜ë¦¬
        depth = output[0, 0]

        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        depth = cv2.resize(depth, (w, h), interpolation=cv2.INTER_CUBIC)

        return depth

    def visualize(self, depth, colormap=cv2.COLORMAP_MAGMA):
        """ê¹Šì´ ë§µ ì‹œê°í™”"""

        # ì •ê·œí™”
        depth_norm = cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX)
        depth_norm = depth_norm.astype(np.uint8)

        # ì»¬ëŸ¬ë§µ ì ìš©
        depth_colored = cv2.applyColorMap(depth_norm, colormap)

        return depth_colored

# ì‚¬ìš© ì˜ˆ
estimator = MiDaSDepthEstimator('midas_v21_384.onnx')

img = cv2.imread('sample.jpg')
depth = estimator.estimate(img)
depth_vis = estimator.visualize(depth)

cv2.imshow('Depth', depth_vis)
cv2.waitKey(0)
```

---

## 3. DPT (Dense Prediction Transformer)

### DPT ì•„í‚¤í…ì²˜

```
DPT (Dense Prediction Transformer):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Vision Transformer (ViT) ê¸°ë°˜ ë°€ì§‘ ì˜ˆì¸¡ ëª¨ë¸                   â”‚
â”‚                                                                 â”‚
â”‚  ì…ë ¥: ì´ë¯¸ì§€ (H Ã— W Ã— 3)                                       â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Patch Embedding                                        â”‚    â”‚
â”‚  â”‚  ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í•  í›„ ì„ë² ë”©                         â”‚    â”‚
â”‚  â”‚  íŒ¨ì¹˜ í¬ê¸°: 16Ã—16                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Transformer Encoder                                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚ Block â”‚â†’â”‚ Block â”‚â†’â”‚ Block â”‚â†’â”‚ Block â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â”‚     â”‚          â”‚          â”‚          â”‚                  â”‚    â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚    â”‚
â”‚  â”‚                â–¼          â–¼          â–¼                  â”‚    â”‚
â”‚  â”‚         ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Reassemble + Fusion                                    â”‚    â”‚
â”‚  â”‚  ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Head (Conv Layers)                                     â”‚    â”‚
â”‚  â”‚  ìµœì¢… ê¹Šì´ ë§µ ì¶œë ¥                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  ì¶œë ¥: ê¹Šì´ ë§µ (H Ã— W)                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DPT êµ¬í˜„

```python
import cv2
import numpy as np
import torch
from torchvision import transforms

class DPTDepthEstimator:
    """DPT ê¹Šì´ ì¶”ì •ê¸°"""

    def __init__(self, model_type='DPT_Large'):
        """
        model_type: 'DPT_Large', 'DPT_Hybrid', 'DPT_SwinV2_L_384'
        """
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )

        # PyTorch Hubì—ì„œ ëª¨ë¸ ë¡œë“œ
        self.model = torch.hub.load('intel-isl/MiDaS', model_type)
        self.model.to(self.device)
        self.model.eval()

        # ì „ì²˜ë¦¬ íŠ¸ëœìŠ¤í¼ ë¡œë“œ
        midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')
        self.transform = midas_transforms.dpt_transform

    def estimate(self, img):
        """ê¹Šì´ ì¶”ì •"""

        h, w = img.shape[:2]

        # BGR â†’ RGB
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
        input_batch = self.transform(img_rgb).to(self.device)

        with torch.no_grad():
            prediction = self.model(input_batch)

            # ì›ë³¸ í¬ê¸°ë¡œ ë³´ê°„
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=(h, w),
                mode='bicubic',
                align_corners=False
            ).squeeze()

        depth = prediction.cpu().numpy()

        return depth

    def get_metric_depth(self, depth, scale=10.0):
        """ìƒëŒ€ ê¹Šì´ â†’ ë¯¸í„° ë‹¨ìœ„ ë³€í™˜ (ê·¼ì‚¬)"""

        # MiDaS/DPTëŠ” ìƒëŒ€ ê¹Šì´ë¥¼ ì¶œë ¥
        # ì ˆëŒ€ ê¹Šì´ë¡œ ë³€í™˜í•˜ë ¤ë©´ ìŠ¤ì¼€ì¼ ì¶”ì • í•„ìš”

        depth_metric = scale / (depth + 1e-6)

        return depth_metric

def estimate_depth_with_confidence(estimator, img, num_samples=5):
    """ëª¬í…Œì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê¹Šì´ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""

    # ì°¸ê³ : ì‹¤ì œë¡œëŠ” ë“œë¡­ì•„ì›ƒì´ ìˆëŠ” ëª¨ë¸ì´ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ëŒ€ì²´

    depths = []

    for _ in range(num_samples):
        # ì•½ê°„ì˜ ì´ë¯¸ì§€ ë³€í˜•
        augmented = img.copy()

        # ë°ê¸° ë³€í™”
        factor = np.random.uniform(0.9, 1.1)
        augmented = np.clip(augmented * factor, 0, 255).astype(np.uint8)

        depth = estimator.estimate(augmented)
        depths.append(depth)

    depths = np.stack(depths, axis=0)

    # í‰ê· ê³¼ í‘œì¤€í¸ì°¨
    mean_depth = np.mean(depths, axis=0)
    std_depth = np.std(depths, axis=0)

    return mean_depth, std_depth
```

### Depth Anything ëª¨ë¸

```python
# Depth Anything: ë” ìµœì‹ ì˜ SOTA ëª¨ë¸

class DepthAnythingEstimator:
    """Depth Anything ëª¨ë¸ (2024)"""

    def __init__(self, model_size='small'):
        """
        model_size: 'small', 'base', 'large'
        """
        from transformers import pipeline

        model_name = f"LiheYoung/depth-anything-{model_size}-hf"
        self.pipe = pipeline(
            task='depth-estimation',
            model=model_name
        )

    def estimate(self, img):
        """ê¹Šì´ ì¶”ì •"""

        # BGR â†’ RGB, PIL ë³€í™˜
        from PIL import Image
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(img_rgb)

        # ì¶”ë¡ 
        result = self.pipe(img_pil)

        # ê¹Šì´ ë§µ ì¶”ì¶œ
        depth = np.array(result['depth'])

        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if depth.shape[:2] != img.shape[:2]:
            depth = cv2.resize(depth, (img.shape[1], img.shape[0]))

        return depth
```

---

## 4. Structure from Motion (SfM)

### SfM ê°œìš”

```
Structure from Motion (SfM):
ì¹´ë©”ë¼ ì›€ì§ì„ì„ ì´ìš©í•´ 3D êµ¬ì¡° ë³µì›

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ì…ë ¥: ì—°ì† ì´ë¯¸ì§€ (ë¹„ë””ì˜¤ ë˜ëŠ” ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€)                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ t=1 â”‚  â”‚ t=2 â”‚  â”‚ t=3 â”‚  â”‚ t=4 â”‚  â”‚ t=5 â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚      â”‚       â”‚       â”‚       â”‚       â”‚                          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚          â”‚  1. íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­   â”‚                          â”‚
â”‚          â”‚     SIFT, ORB, SuperPoint â”‚                          â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚          â”‚  2. ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì •      â”‚                          â”‚
â”‚          â”‚     Essential Matrix      â”‚                          â”‚
â”‚          â”‚     PnP                   â”‚                          â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚          â”‚  3. ì‚¼ê°ì¸¡ëŸ‰              â”‚                          â”‚
â”‚          â”‚     3D ì  ë³µì›            â”‚                          â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚          â”‚  4. ë²ˆë“¤ ì¡°ì •             â”‚                          â”‚
â”‚          â”‚     ì „ì—­ ìµœì í™”           â”‚                          â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚  ì¶œë ¥: 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ + ì¹´ë©”ë¼ ê¶¤ì                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SfM êµ¬í˜„ (ê°„ë‹¨í•œ ë²„ì „)

```python
import cv2
import numpy as np

class SimpleSfM:
    """ê°„ë‹¨í•œ 2-ë·° SfM êµ¬í˜„"""

    def __init__(self, K):
        """
        K: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬
        """
        self.K = K
        self.sift = cv2.SIFT_create()
        self.bf = cv2.BFMatcher()

    def detect_and_match(self, img1, img2):
        """íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­"""

        # íŠ¹ì§•ì  ê²€ì¶œ
        kp1, desc1 = self.sift.detectAndCompute(img1, None)
        kp2, desc2 = self.sift.detectAndCompute(img2, None)

        # ë§¤ì¹­
        matches = self.bf.knnMatch(desc1, desc2, k=2)

        # ë¹„ìœ¨ í…ŒìŠ¤íŠ¸
        good_matches = []
        for m, n in matches:
            if m.distance < 0.75 * n.distance:
                good_matches.append(m)

        # ë§¤ì¹­ì  ì¢Œí‘œ
        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])
        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])

        return pts1, pts2, good_matches, kp1, kp2

    def estimate_pose(self, pts1, pts2):
        """Essential Matrixë¡œ ìƒëŒ€ í¬ì¦ˆ ì¶”ì •"""

        E, mask = cv2.findEssentialMat(
            pts1, pts2, self.K,
            method=cv2.RANSAC,
            prob=0.999,
            threshold=1.0
        )

        # R, t ë³µêµ¬
        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, self.K, mask)

        return R, t, mask.ravel().astype(bool)

    def triangulate(self, pts1, pts2, R, t):
        """ì‚¼ê°ì¸¡ëŸ‰ìœ¼ë¡œ 3D ì  ë³µì›"""

        # íˆ¬ì˜ í–‰ë ¬
        P1 = self.K @ np.hstack([np.eye(3), np.zeros((3, 1))])
        P2 = self.K @ np.hstack([R, t])

        # ì‚¼ê°ì¸¡ëŸ‰
        pts1_h = pts1.T  # (2, N)
        pts2_h = pts2.T

        points_4d = cv2.triangulatePoints(P1, P2, pts1_h, pts2_h)

        # ë™ì°¨ ì¢Œí‘œ â†’ ìœ í´ë¦¬ë“œ ì¢Œí‘œ
        points_3d = points_4d[:3] / points_4d[3]

        return points_3d.T  # (N, 3)

    def filter_points(self, pts1, pts2, points_3d, R, t):
        """ìœ íš¨í•œ 3D ì  í•„í„°ë§"""

        # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        P2 = self.K @ np.hstack([R, t])

        projected = P2 @ np.hstack([points_3d, np.ones((len(points_3d), 1))]).T
        projected = projected[:2] / projected[2]
        projected = projected.T

        errors = np.linalg.norm(pts2 - projected, axis=1)

        # ì¹´ë©”ë¼ ì•ì— ìˆëŠ”ì§€ í™•ì¸
        # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ ê¸°ì¤€
        valid_depth1 = points_3d[:, 2] > 0

        # ë‘ ë²ˆì§¸ ì¹´ë©”ë¼ ê¸°ì¤€
        points_cam2 = (R @ points_3d.T + t).T
        valid_depth2 = points_cam2[:, 2] > 0

        # ì¬íˆ¬ì˜ ì˜¤ì°¨ ì„ê³„ê°’
        valid_reproj = errors < 2.0

        valid = valid_depth1 & valid_depth2 & valid_reproj

        return points_3d[valid], valid

    def run(self, img1, img2):
        """ì „ì²´ SfM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        # 1. íŠ¹ì§•ì  ë§¤ì¹­
        pts1, pts2, matches, kp1, kp2 = self.detect_and_match(img1, img2)
        print(f"ë§¤ì¹­ì  ìˆ˜: {len(pts1)}")

        # 2. í¬ì¦ˆ ì¶”ì •
        R, t, inlier_mask = self.estimate_pose(pts1, pts2)
        pts1 = pts1[inlier_mask]
        pts2 = pts2[inlier_mask]
        print(f"ì¸ë¼ì´ì–´ ìˆ˜: {len(pts1)}")

        # 3. ì‚¼ê°ì¸¡ëŸ‰
        points_3d = self.triangulate(pts1, pts2, R, t)

        # 4. í•„í„°ë§
        points_3d, valid = self.filter_points(pts1, pts2, points_3d, R, t)
        print(f"ìœ íš¨í•œ 3D ì  ìˆ˜: {len(points_3d)}")

        return points_3d, R, t

# ì‚¬ìš© ì˜ˆ
K = np.array([
    [800, 0, 320],
    [0, 800, 240],
    [0, 0, 1]
], dtype=np.float32)

sfm = SimpleSfM(K)
img1 = cv2.imread('image1.jpg')
img2 = cv2.imread('image2.jpg')
points_3d, R, t = sfm.run(img1, img2)
```

### ë‹¤ì¤‘ ë·° SfM

```python
class IncrementalSfM:
    """ì¦ë¶„ì  SfM"""

    def __init__(self, K):
        self.K = K
        self.sift = cv2.SIFT_create(nfeatures=8000)
        self.bf = cv2.BFMatcher()

        # ì „ì—­ ë°ì´í„°
        self.points_3d = None
        self.point_colors = None
        self.camera_poses = []
        self.keypoints_all = []
        self.descriptors_all = []

    def add_image(self, img):
        """ìƒˆ ì´ë¯¸ì§€ ì¶”ê°€"""

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        kp, desc = self.sift.detectAndCompute(gray, None)

        self.keypoints_all.append(kp)
        self.descriptors_all.append(desc)

        return len(self.keypoints_all) - 1

    def initialize(self, idx1, idx2):
        """ì²« ë‘ ì´ë¯¸ì§€ë¡œ ì´ˆê¸°í™”"""

        # ë§¤ì¹­
        matches = self.bf.knnMatch(
            self.descriptors_all[idx1],
            self.descriptors_all[idx2],
            k=2
        )

        good = [m for m, n in matches if m.distance < 0.7 * n.distance]

        pts1 = np.float32([self.keypoints_all[idx1][m.queryIdx].pt for m in good])
        pts2 = np.float32([self.keypoints_all[idx2][m.trainIdx].pt for m in good])

        # Essential Matrix
        E, mask = cv2.findEssentialMat(pts1, pts2, self.K)
        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, self.K)

        mask = mask.ravel().astype(bool)
        pts1 = pts1[mask]
        pts2 = pts2[mask]

        # ì‚¼ê°ì¸¡ëŸ‰
        P1 = self.K @ np.hstack([np.eye(3), np.zeros((3, 1))])
        P2 = self.K @ np.hstack([R, t])

        points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)
        self.points_3d = (points_4d[:3] / points_4d[3]).T

        # ì¹´ë©”ë¼ í¬ì¦ˆ ì €ì¥
        self.camera_poses = [
            {'R': np.eye(3), 't': np.zeros((3, 1))},
            {'R': R, 't': t}
        ]

        print(f"ì´ˆê¸°í™” ì™„ë£Œ: {len(self.points_3d)} 3D ì ")

    def register_image(self, idx):
        """ìƒˆ ì´ë¯¸ì§€ ë“±ë¡ (PnP)"""

        if self.points_3d is None or len(self.points_3d) == 0:
            print("ë¨¼ì € ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return False

        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ëœ ì´ë¯¸ì§€ì™€ ë§¤ì¹­
        last_idx = len(self.camera_poses) - 1

        matches = self.bf.knnMatch(
            self.descriptors_all[last_idx],
            self.descriptors_all[idx],
            k=2
        )

        good = [m for m, n in matches if m.distance < 0.7 * n.distance]

        if len(good) < 8:
            print("ë§¤ì¹­ì  ë¶€ì¡±")
            return False

        # 3D-2D ëŒ€ì‘ì  (ë‹¨ìˆœí™”: ì´ì „ ì´ë¯¸ì§€ì˜ ë§¤ì¹­ì  ì¸ë±ìŠ¤ ì‚¬ìš©)
        # ì‹¤ì œë¡œëŠ” íŠ¸ë™ ê´€ë¦¬ í•„ìš”
        obj_points = []
        img_points = []

        for m in good[:len(self.points_3d)]:
            if m.queryIdx < len(self.points_3d):
                obj_points.append(self.points_3d[m.queryIdx])
                img_points.append(
                    self.keypoints_all[idx][m.trainIdx].pt
                )

        if len(obj_points) < 6:
            print("ëŒ€ì‘ì  ë¶€ì¡±")
            return False

        obj_points = np.array(obj_points, dtype=np.float32)
        img_points = np.array(img_points, dtype=np.float32)

        # PnP
        success, rvec, tvec, inliers = cv2.solvePnPRansac(
            obj_points, img_points, self.K, None
        )

        if not success:
            print("PnP ì‹¤íŒ¨")
            return False

        R, _ = cv2.Rodrigues(rvec)
        self.camera_poses.append({'R': R, 't': tvec})

        print(f"ì´ë¯¸ì§€ {idx} ë“±ë¡ ì™„ë£Œ")
        return True

    def bundle_adjust(self):
        """ë²ˆë“¤ ì¡°ì • (scipy ì‚¬ìš©)"""

        from scipy.optimize import least_squares

        # ê°„ë‹¨í•œ ë²ˆë“¤ ì¡°ì • êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” g2o, Ceres ë“± ì‚¬ìš© ê¶Œì¥

        print("ë²ˆë“¤ ì¡°ì •ì€ ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¶Œì¥ (g2o, Ceres)")

    def get_point_cloud(self):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë°˜í™˜"""
        return self.points_3d

    def get_camera_trajectory(self):
        """ì¹´ë©”ë¼ ê¶¤ì  ë°˜í™˜"""
        positions = []
        for pose in self.camera_poses:
            R = pose['R']
            t = pose['t']
            # ì¹´ë©”ë¼ ìœ„ì¹˜ = -R^T * t
            pos = -R.T @ t
            positions.append(pos.ravel())

        return np.array(positions)
```

---

## 5. ê¹Šì´ ë§µ ì‘ìš©

### ê¹Šì´ ê¸°ë°˜ ì´ë¯¸ì§€ íš¨ê³¼

```python
import cv2
import numpy as np

def apply_bokeh_effect(img, depth, focus_depth=0.5, aperture=0.1):
    """ê¹Šì´ ê¸°ë°˜ ë³´ì¼€ íš¨ê³¼ (í”¼ì‚¬ê³„ ì‹¬ë„ ì‹œë®¬ë ˆì´ì…˜)"""

    # ê¹Šì´ ì •ê·œí™” (0-1)
    depth_norm = (depth - depth.min()) / (depth.max() - depth.min())

    # ì´ˆì  ê±°ë¦¬ì—ì„œì˜ í¸ì°¨ ê³„ì‚°
    depth_diff = np.abs(depth_norm - focus_depth)

    # ë¸”ëŸ¬ ê°•ë„ (ì´ˆì ì—ì„œ ë©€ìˆ˜ë¡ ê°•í•¨)
    blur_strength = (depth_diff / aperture * 30).astype(int)
    blur_strength = np.clip(blur_strength, 0, 31)

    # ë¸”ëŸ¬ ì ìš© (í”½ì…€ë³„ë¡œ ë‹¤ë¥¸ ê°•ë„)
    result = np.zeros_like(img, dtype=np.float32)

    for blur_level in range(0, 32, 2):
        mask = (blur_strength >= blur_level) & (blur_strength < blur_level + 2)

        if blur_level == 0:
            blurred = img.astype(np.float32)
        else:
            ksize = blur_level * 2 + 1
            blurred = cv2.GaussianBlur(img, (ksize, ksize), 0).astype(np.float32)

        result += blurred * mask[:, :, np.newaxis]

    return result.astype(np.uint8)

def create_depth_fog(img, depth, fog_color=(200, 200, 200), max_fog=0.8):
    """ê¹Šì´ ê¸°ë°˜ ì•ˆê°œ íš¨ê³¼"""

    # ê¹Šì´ ì •ê·œí™”
    depth_norm = (depth - depth.min()) / (depth.max() - depth.min())

    # ì•ˆê°œ ê°•ë„ (ë©€ìˆ˜ë¡ ê°•í•¨)
    fog_factor = depth_norm * max_fog

    # ì•ˆê°œ ì ìš©
    fog = np.full_like(img, fog_color, dtype=np.float32)
    result = img.astype(np.float32) * (1 - fog_factor[:, :, np.newaxis])
    result += fog * fog_factor[:, :, np.newaxis]

    return result.astype(np.uint8)

def depth_based_segmentation(img, depth, num_layers=5):
    """ê¹Šì´ ê¸°ë°˜ ë ˆì´ì–´ ë¶„í• """

    # ê¹Šì´ ì •ê·œí™”
    depth_norm = (depth - depth.min()) / (depth.max() - depth.min())

    # ê¹Šì´ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
    layers = []
    for i in range(num_layers):
        lower = i / num_layers
        upper = (i + 1) / num_layers
        mask = (depth_norm >= lower) & (depth_norm < upper)

        layer = np.zeros_like(img)
        layer[mask] = img[mask]
        layers.append(layer)

    return layers

def remove_background_with_depth(img, depth, threshold=0.5):
    """ê¹Šì´ ê¸°ë°˜ ë°°ê²½ ì œê±°"""

    # ê¹Šì´ ì •ê·œí™”
    depth_norm = (depth - depth.min()) / (depth.max() - depth.min())

    # ì „ê²½ ë§ˆìŠ¤í¬ (ì„ê³„ê°’ë³´ë‹¤ ê°€ê¹Œìš´ ë¶€ë¶„)
    foreground_mask = depth_norm < threshold

    # ë§ˆìŠ¤í¬ ì •ì œ
    kernel = np.ones((5, 5), np.uint8)
    foreground_mask = cv2.morphologyEx(
        foreground_mask.astype(np.uint8),
        cv2.MORPH_CLOSE, kernel
    )
    foreground_mask = cv2.morphologyEx(
        foreground_mask,
        cv2.MORPH_OPEN, kernel
    )

    # ë°°ê²½ ì œê±°
    result = np.zeros_like(img)
    result[foreground_mask == 1] = img[foreground_mask == 1]

    return result, foreground_mask
```

### 3D íš¨ê³¼ ìƒì„±

```python
def create_3d_ken_burns(img, depth, num_frames=60, zoom=0.1):
    """Ken Burns íš¨ê³¼ (3D ì¹´ë©”ë¼ ì›€ì§ì„)"""

    h, w = img.shape[:2]
    frames = []

    for i in range(num_frames):
        t = i / (num_frames - 1)

        # ì¤Œ íŒ©í„°
        scale = 1 + zoom * t

        # ê¹Šì´ì— ë”°ë¥¸ ì‹œì°¨
        parallax = (depth - depth.mean()) * 0.001 * t

        # ìƒˆ ì¢Œí‘œ ê³„ì‚°
        y_coords, x_coords = np.meshgrid(range(h), range(w), indexing='ij')

        # ì¤‘ì‹¬ ê¸°ì¤€ ìŠ¤ì¼€ì¼ë§
        new_x = (x_coords - w/2) / scale + w/2 + parallax
        new_y = (y_coords - h/2) / scale + h/2

        # ë¦¬ë§µí•‘
        map_x = new_x.astype(np.float32)
        map_y = new_y.astype(np.float32)

        frame = cv2.remap(img, map_x, map_y, cv2.INTER_LINEAR)
        frames.append(frame)

    return frames

def depth_aware_zoom(img, depth, zoom_center, zoom_factor=2.0):
    """ê¹Šì´ ì¸ì‹ ì¤Œ"""

    h, w = img.shape[:2]
    cx, cy = zoom_center

    # ê¹Šì´ ì •ê·œí™”
    depth_norm = (depth - depth.min()) / (depth.max() - depth.min())

    # ê¹Šì´ì— ë”°ë¼ ë‹¤ë¥¸ ì¤Œ ì ìš© (ê°€ê¹Œìš´ ë¬¼ì²´ëŠ” ë” ë§ì´ í™•ëŒ€)
    depth_factor = 1 - depth_norm * 0.5  # 0.5 ~ 1.0

    # ì¢Œí‘œ ê·¸ë¦¬ë“œ
    y_coords, x_coords = np.meshgrid(range(h), range(w), indexing='ij')

    # ì¤Œ ë³€í™˜ (ê¹Šì´ë³„ë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼)
    effective_zoom = zoom_factor * depth_factor

    new_x = (x_coords - cx) / effective_zoom + cx
    new_y = (y_coords - cy) / effective_zoom + cy

    # ë¦¬ë§µí•‘
    map_x = new_x.astype(np.float32)
    map_y = new_y.astype(np.float32)

    result = cv2.remap(img, map_x, map_y, cv2.INTER_LINEAR)

    return result
```

---

## 6. ì—°ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: MiDaS ê¹Šì´ ì¶”ì •

MiDaSë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ê¹Šì´ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
- ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
- ê¹Šì´ ë§µ ì‹œê°í™” (ì»¬ëŸ¬ë§µ)
- ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸

<details>
<summary>íŒíŠ¸</summary>

```python
import torch

model = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')
midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')
transform = midas_transforms.dpt_transform
```

</details>

### ë¬¸ì œ 2: ê¹Šì´ ê¸°ë°˜ ë°°ê²½ ë¸”ëŸ¬

ì¸ë¬¼ ì‚¬ì§„ì—ì„œ ë°°ê²½ë§Œ ë¸”ëŸ¬ ì²˜ë¦¬í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
- ê¹Šì´ ì¶”ì •
- ì „ê²½/ë°°ê²½ ë¶„ë¦¬
- ë°°ê²½ì—ë§Œ ë¸”ëŸ¬ ì ìš©
- ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ ì²˜ë¦¬

<details>
<summary>íŒíŠ¸</summary>

```python
# ê¹Šì´ ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„±
threshold = np.percentile(depth, 30)  # ê°€ê¹Œìš´ 30%ë¥¼ ì „ê²½ìœ¼ë¡œ
foreground_mask = depth < threshold

# ë§ˆìŠ¤í¬ ë¸”ëŸ¬ë§ (ê²½ê³„ ë¶€ë“œëŸ½ê²Œ)
mask_blur = cv2.GaussianBlur(
    foreground_mask.astype(np.float32), (21, 21), 0
)

# ë°°ê²½ ë¸”ëŸ¬
background_blur = cv2.GaussianBlur(img, (25, 25), 0)

# í•©ì„±
result = img * mask_blur[..., None] + background_blur * (1 - mask_blur[..., None])
```

</details>

### ë¬¸ì œ 3: SfMìœ¼ë¡œ 3D ë³µì›

ë‘ ì´ë¯¸ì§€ì—ì„œ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ë³µì›í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
- íŠ¹ì§•ì  ë§¤ì¹­
- Essential Matrix ê³„ì‚°
- ì‚¼ê°ì¸¡ëŸ‰
- í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì‹œê°í™”

<details>
<summary>íŒíŠ¸</summary>

```python
# Essential Matrix
E, mask = cv2.findEssentialMat(pts1, pts2, K)
_, R, t, _ = cv2.recoverPose(E, pts1, pts2, K)

# íˆ¬ì˜ í–‰ë ¬
P1 = K @ np.hstack([np.eye(3), np.zeros((3, 1))])
P2 = K @ np.hstack([R, t])

# ì‚¼ê°ì¸¡ëŸ‰
points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)
points_3d = points_4d[:3] / points_4d[3]
```

</details>

### ë¬¸ì œ 4: ì‹¤ì‹œê°„ ê¹Šì´ ì¶”ì •

ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ê¹Šì´ ì¶”ì •ì„ êµ¬í˜„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
- ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš© (MiDaS small)
- FPS ì¸¡ì • ë° í‘œì‹œ
- ê¹Šì´ ì‹œê°í™”

<details>
<summary>íŒíŠ¸</summary>

```python
# ê²½ëŸ‰ ëª¨ë¸
model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')

while True:
    ret, frame = cap.read()

    start = time.time()
    depth = estimate_depth(frame, model, transform)
    fps = 1.0 / (time.time() - start)

    cv2.putText(depth_vis, f"FPS: {fps:.1f}", ...)
```

</details>

### ë¬¸ì œ 5: ê¹Šì´ ê¸°ë°˜ 3D ë·°ì–´

ê¹Šì´ ë§µì„ ì´ìš©í•´ ê°„ë‹¨í•œ 3D ë·°ì–´ë¥¼ ë§Œë“œì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
- ê¹Šì´ ë§µ â†’ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë³€í™˜
- Open3Dë¡œ ì‹œê°í™”
- ë§ˆìš°ìŠ¤ë¡œ íšŒì „/ì¤Œ

<details>
<summary>íŒíŠ¸</summary>

```python
import open3d as o3d

# í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
pcd = o3d.geometry.PointCloud()
pcd.points = o3d.utility.Vector3dVector(points_3d)
pcd.colors = o3d.utility.Vector3dVector(colors / 255.0)

# ì‹œê°í™”
o3d.visualization.draw_geometries([pcd])
```

</details>

---

## ë‹¤ìŒ ë‹¨ê³„

- [23_SLAM_Introduction.md](./23_SLAM_Introduction.md) - Visual SLAM, ORB-SLAM, LiDAR SLAM, Loop Closure

---

## ì°¸ê³  ìë£Œ

- [MiDaS GitHub](https://github.com/isl-org/MiDaS)
- [DPT Paper](https://arxiv.org/abs/2103.13413)
- [Depth Anything](https://github.com/LiheYoung/Depth-Anything)
- [Structure from Motion Tutorial](https://cmsc426.github.io/sfm/)
- [OpenCV SfM Tutorial](https://docs.opencv.org/4.x/d4/d18/tutorial_sfm_scene_reconstruction.html)
- [Monodepth2](https://github.com/nianticlabs/monodepth2)
