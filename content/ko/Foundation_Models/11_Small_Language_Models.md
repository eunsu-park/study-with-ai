# 11. Small Language Models

## ê°œìš”

ëŒ€í˜• ëª¨ë¸(100B+)ì´ í™”ì œì§€ë§Œ, ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” **Small Language Models (SLM)**ì´ ë” ì‹¤ìš©ì ì…ë‹ˆë‹¤. ì´ ë ˆìŠ¨ì—ì„œëŠ” 7B ì´í•˜ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜, í•™ìŠµ ì „ëµ, í™œìš© ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## 1. SLMì˜ ì¤‘ìš”ì„±

### 1.1 ì™œ ì‘ì€ ëª¨ë¸ì¸ê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SLM vs LLM ë¹„êµ                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚                    SLM (1-7B)              LLM (70B+)            â”‚
â”‚                                                                  â”‚
â”‚  ğŸ’° ë¹„ìš©          ë‚®ìŒ                      ë†’ìŒ                 â”‚
â”‚  âš¡ ì§€ì—°ì‹œê°„      ë‚®ìŒ (<100ms)             ë†’ìŒ (>500ms)        â”‚
â”‚  ğŸ–¥ï¸ í•˜ë“œì›¨ì–´     ë‹¨ì¼ GPU/CPU             ë‹¤ì¤‘ GPU í•„ìˆ˜        â”‚
â”‚  ğŸ“± ì—£ì§€ ë°°í¬    ê°€ëŠ¥                      ì–´ë ¤ì›€               â”‚
â”‚  ğŸ”’ í”„ë¼ì´ë²„ì‹œ   ì˜¨í”„ë ˆë¯¸ìŠ¤ ì‰¬ì›€           ì–´ë ¤ì›€               â”‚
â”‚  ğŸ¯ íŠ¹í™” íƒœìŠ¤í¬  ë¹„ìš© íš¨ìœ¨ì                ê³¼ì‰                 â”‚
â”‚                                                                  â”‚
â”‚  ì‚¬ìš© ì‚¬ë¡€:                                                      â”‚
â”‚  - ëª¨ë°”ì¼ ì•± (On-device)                                        â”‚
â”‚  - ì„ë² ë””ë“œ ì‹œìŠ¤í…œ                                              â”‚
â”‚  - ê³ ë¹ˆë„ API ì„œë¹„ìŠ¤                                            â”‚
â”‚  - ë¹„ìš© ë¯¼ê°í•œ ìŠ¤íƒ€íŠ¸ì—…                                         â”‚
â”‚  - ê°œì¸ì •ë³´ ë³´í˜¸ê°€ ì¤‘ìš”í•œ ë„ë©”ì¸                                â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 SLM ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | í•™ìŠµ í† í° | íŠ¹ì§• |
|------|----------|-----------|------|
| **Phi-3** | 3.8B | 3.3T | MS, ì¶”ë¡  íŠ¹í™” |
| **Gemma 2** | 2B / 9B | 8T | Google, ì½”ë“œ ê°•ì  |
| **Qwen 2.5** | 0.5B - 7B | 18T | ë‹¤êµ­ì–´, ìˆ˜í•™ |
| **Llama 3.2** | 1B / 3B | 15T | ëª¨ë°”ì¼ ìµœì í™” |
| **TinyLlama** | 1.1B | 3T | íš¨ìœ¨ì  í•™ìŠµ |
| **StableLM 2** | 1.6B | 2T | Stability AI |
| **SmolLM** | 135M - 1.7B | 1T | HuggingFace |

---

## 2. ì•„í‚¤í…ì²˜ ìµœì í™”

### 2.1 Phi ì‹œë¦¬ì¦ˆ (Microsoft)

```python
"""
Phi-3: "Textbooks Are All You Need" ì² í•™

í•µì‹¬ ì•„ì´ë””ì–´:
1. ë°ì´í„° í’ˆì§ˆ > ë°ì´í„° ì–‘
2. í•©ì„± ë°ì´í„° í™œìš© (GPT-4ë¡œ ìƒì„±)
3. êµê³¼ì„œê¸‰ í’ˆì§ˆì˜ ë°ì´í„°ë§Œ ì‚¬ìš©

ê²°ê³¼: 3.8Bë¡œ GPT-3.5ê¸‰ ì¶”ë¡  ëŠ¥ë ¥
"""

class Phi3Config:
    """Phi-3 ì•„í‚¤í…ì²˜ ì„¤ì •"""

    # Phi-3-mini (3.8B)
    hidden_size = 3072
    num_layers = 32
    num_attention_heads = 32
    num_key_value_heads = 32  # No GQA
    intermediate_size = 8192  # FFN í™•ì¥ë¹„ ~2.7x
    vocab_size = 32064
    max_position_embeddings = 4096  # í™•ì¥ ê°€ëŠ¥

    # íŠ¹ì§•
    # - SuRoPE (Scaled RoPE)
    # - LayerNorm (RMSNorm ëŒ€ì‹ )
    # - SwiGLU FFN


# Phi-3 ì‚¬ìš© ì˜ˆì‹œ
from transformers import AutoModelForCausalLM, AutoTokenizer

def use_phi3():
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-4k-instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "microsoft/Phi-3-mini-4k-instruct"
    )

    # ì¶”ë¡ 
    messages = [
        {"role": "user", "content": "Explain the Pythagorean theorem."}
    ]

    inputs = tokenizer.apply_chat_template(
        messages, return_tensors="pt", return_dict=True
    ).to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7
    )

    return tokenizer.decode(outputs[0])
```

### 2.2 Gemma 2 (Google)

```python
"""
Gemma 2: íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

í•µì‹¬ íŠ¹ì§•:
1. Alternating Local-Global Attention
2. Soft-Capping (Logits & Attention)
3. Pre-Norm + Post-Norm hybrid
4. Knowledge Distillation from larger models
"""

class Gemma2Config:
    """Gemma 2 ì•„í‚¤í…ì²˜"""

    # Gemma 2 2B
    hidden_size = 2304
    num_layers = 26
    num_attention_heads = 8
    num_key_value_heads = 4  # GQA ì‚¬ìš©
    intermediate_size = 9216
    vocab_size = 256128  # í° vocab

    # Gemma 2 9B
    # hidden_size = 3584
    # num_layers = 42
    # num_attention_heads = 16
    # num_key_value_heads = 8


class GemmaAttentionWithSoftCap(nn.Module):
    """Gemma 2 ìŠ¤íƒ€ì¼ Soft-Capping Attention"""

    def __init__(self, config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        # Local vs Global attention êµëŒ€
        # ì§ìˆ˜ ë ˆì´ì–´: Local (sliding window)
        # í™€ìˆ˜ ë ˆì´ì–´: Global (full attention)
        self.is_local = (layer_idx % 2 == 0)
        self.sliding_window = 4096 if self.is_local else None

        # Soft-cap ê°’
        self.attn_logit_softcap = 50.0

        # Projections
        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)
        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size // 2)  # GQA
        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size // 2)
        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size)

    def forward(self, hidden_states, attention_mask=None):
        batch, seq_len, _ = hidden_states.shape

        Q = self.q_proj(hidden_states)
        K = self.k_proj(hidden_states)
        V = self.v_proj(hidden_states)

        # GQA: K, V í™•ì¥
        K = K.repeat_interleave(2, dim=-1)  # ê°„ì†Œí™”
        V = V.repeat_interleave(2, dim=-1)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1))
        scores = scores / math.sqrt(Q.shape[-1])

        # Soft-capping: tanhë¡œ ë²”ìœ„ ì œí•œ
        scores = self.attn_logit_softcap * torch.tanh(scores / self.attn_logit_softcap)

        # Sliding window mask (local attention)
        if self.is_local and self.sliding_window:
            mask = self._create_sliding_window_mask(seq_len)
            scores = scores + mask

        # Causal mask
        causal_mask = torch.triu(
            torch.ones(seq_len, seq_len) * float('-inf'),
            diagonal=1
        ).to(scores.device)
        scores = scores + causal_mask

        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)

        return self.o_proj(output)

    def _create_sliding_window_mask(self, seq_len):
        """Sliding window attention mask"""
        mask = torch.ones(seq_len, seq_len) * float('-inf')
        for i in range(seq_len):
            start = max(0, i - self.sliding_window)
            mask[i, start:i+1] = 0
        return mask
```

### 2.3 Qwen 2.5 (Alibaba)

```python
"""
Qwen 2.5: ë‹¤êµ­ì–´ & ìˆ˜í•™ ê°•ì 

íŠ¹ì§•:
1. ëŒ€ê·œëª¨ ë‹¤êµ­ì–´ í•™ìŠµ (29ê°œ ì–¸ì–´)
2. ì½”ë“œ/ìˆ˜í•™ íŠ¹í™” ë°ì´í„°
3. ê¸´ ì»¨í…ìŠ¤íŠ¸ (128K)
4. ë‹¤ì–‘í•œ í¬ê¸° (0.5B ~ 72B)
"""

class Qwen25Config:
    """Qwen 2.5 ì•„í‚¤í…ì²˜"""

    # Qwen2.5-0.5B (ê°€ì¥ ì‘ì€ ë²„ì „)
    hidden_size = 896
    num_layers = 24
    num_attention_heads = 14
    num_key_value_heads = 2  # íš¨ìœ¨ì  GQA
    intermediate_size = 4864
    vocab_size = 151936

    # Qwen2.5-7B
    # hidden_size = 3584
    # num_layers = 28
    # num_attention_heads = 28
    # num_key_value_heads = 4


# Qwen ì‚¬ìš© ì˜ˆì‹œ
def use_qwen():
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-0.5B-Instruct",
        torch_dtype="auto",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

    # ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸
    prompts = [
        "Explain machine learning in simple terms.",
        "ç”¨ç®€å•çš„è¯è§£é‡Šæœºå™¨å­¦ä¹ ",  # ì¤‘êµ­ì–´
        "ê¸°ê³„ í•™ìŠµì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”",  # í•œêµ­ì–´
    ]

    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=128)
        print(tokenizer.decode(outputs[0], skip_special_tokens=True))
        print("-" * 50)
```

---

## 3. í•™ìŠµ ì „ëµ

### 3.1 ë°ì´í„° í’ˆì§ˆ vs ì–‘

```python
"""
SLM í•™ìŠµì˜ í•µì‹¬: ê³ í’ˆì§ˆ ë°ì´í„°

Phiì˜ êµí›ˆ:
- ì›¹ í¬ë¡¤ë§ ë°ì´í„° (í’ˆì§ˆ ë‚®ìŒ) < êµê³¼ì„œê¸‰ ë°ì´í„°
- í•©ì„± ë°ì´í„° (GPT-4 ìƒì„±)ê°€ íš¨ê³¼ì 
- í•„í„°ë§ì´ ë§¤ìš° ì¤‘ìš”
"""

class HighQualityDataPipeline:
    """ê³ í’ˆì§ˆ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""

    def __init__(self, quality_model):
        self.quality_model = quality_model

    def filter_data(self, texts: list, threshold: float = 0.8):
        """í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§"""
        filtered = []
        for text in texts:
            score = self.quality_model.score(text)
            if score > threshold:
                filtered.append(text)

        print(f"Filtered: {len(texts)} â†’ {len(filtered)}")
        return filtered

    def generate_synthetic_data(
        self,
        teacher_model,
        topics: list,
        n_samples: int = 10000
    ):
        """í•©ì„± ë°ì´í„° ìƒì„±"""
        synthetic_data = []

        for topic in topics:
            prompt = f"""Create an educational explanation about {topic}.
            The explanation should be:
            1. Clear and concise
            2. Include examples
            3. Suitable for learning"""

            for _ in range(n_samples // len(topics)):
                response = teacher_model.generate(prompt)

                # í’ˆì§ˆ ê²€ì¦
                if self._validate_response(response):
                    synthetic_data.append({
                        'topic': topic,
                        'content': response
                    })

        return synthetic_data

    def _validate_response(self, response: str) -> bool:
        """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦"""
        # ê¸¸ì´ ì²´í¬
        if len(response.split()) < 50:
            return False

        # ë°˜ë³µ ì²´í¬
        sentences = response.split('.')
        if len(set(sentences)) / len(sentences) < 0.8:
            return False

        return True
```

### 3.2 Knowledge Distillation

```python
"""
Knowledge Distillation: í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸

Teacher (ëŒ€í˜• ëª¨ë¸)ì˜ ì§€ì‹ì„ Student (SLM)ì—ê²Œ ì „ë‹¬
"""

class DistillationTrainer:
    """KD ê¸°ë°˜ SLM í•™ìŠµ"""

    def __init__(
        self,
        teacher_model,  # ì˜ˆ: Llama 70B
        student_model,  # ì˜ˆ: 3B ëª¨ë¸
        temperature: float = 2.0,
        alpha: float = 0.5  # soft/hard loss ë¹„ìœ¨
    ):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha

        # TeacherëŠ” í•™ìŠµ ì•ˆ í•¨
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False

    def distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        Distillation Loss = Î± Ã— Soft Loss + (1-Î±) Ã— Hard Loss

        Soft Loss: KL(student_soft || teacher_soft)
        Hard Loss: CrossEntropy(student, labels)
        """
        T = self.temperature

        # Soft targets (temperature scaling)
        teacher_soft = F.softmax(teacher_logits / T, dim=-1)
        student_soft = F.log_softmax(student_logits / T, dim=-1)

        # KL Divergence (soft loss)
        soft_loss = F.kl_div(
            student_soft,
            teacher_soft,
            reduction='batchmean'
        ) * (T ** 2)  # Temperature scaling ë³´ì •

        # Cross Entropy (hard loss)
        hard_loss = F.cross_entropy(
            student_logits.view(-1, student_logits.size(-1)),
            labels.view(-1),
            ignore_index=-100
        )

        # Combined loss
        loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss

        return loss

    def train_step(self, batch):
        """í•™ìŠµ ìŠ¤í…"""
        input_ids = batch['input_ids']
        labels = batch['labels']

        # Teacher forward (no grad)
        with torch.no_grad():
            teacher_outputs = self.teacher(input_ids)
            teacher_logits = teacher_outputs.logits

        # Student forward
        student_outputs = self.student(input_ids)
        student_logits = student_outputs.logits

        # Distillation loss
        loss = self.distillation_loss(
            student_logits, teacher_logits, labels
        )

        return loss


# Response-level Distillation (ë” íš¨ê³¼ì )
class ResponseDistillation:
    """ì‘ë‹µ ìˆ˜ì¤€ KD"""

    def __init__(self, teacher_model, student_model):
        self.teacher = teacher_model
        self.student = student_model

    def generate_training_data(self, prompts: list):
        """Teacher ì‘ë‹µìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±"""
        training_data = []

        for prompt in prompts:
            # Teacher ì‘ë‹µ ìƒì„±
            teacher_response = self.teacher.generate(
                prompt,
                max_new_tokens=512,
                temperature=0.7
            )

            training_data.append({
                'prompt': prompt,
                'response': teacher_response
            })

        return training_data

    def train_on_responses(self, training_data):
        """Teacher ì‘ë‹µìœ¼ë¡œ Student í•™ìŠµ"""
        # Standard SFT (Supervised Fine-Tuning)
        for item in training_data:
            full_text = f"{item['prompt']}\n{item['response']}"
            # ... SFT í•™ìŠµ
```

### 3.3 íš¨ìœ¨ì  í•™ìŠµ ê¸°ë²•

```python
"""
SLM í•™ìŠµ íš¨ìœ¨í™” ê¸°ë²•
"""

# 1. Gradient Accumulation (ì‘ì€ ë°°ì¹˜ë¡œ í° effective batch)
def train_with_grad_accumulation(
    model,
    dataloader,
    accumulation_steps: int = 8
):
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for i, batch in enumerate(dataloader):
        outputs = model(**batch)
        loss = outputs.loss / accumulation_steps
        loss.backward()

        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()


# 2. LoRAë¡œ íš¨ìœ¨ì  fine-tuning
from peft import LoraConfig, get_peft_model

def setup_lora_training(model):
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none"
    )

    model = get_peft_model(model, lora_config)

    # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í™•ì¸
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")

    return model


# 3. QLoRA (ì–‘ìí™” + LoRA)
from transformers import BitsAndBytesConfig

def setup_qlora_training(model_name):
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto"
    )

    # LoRA ì¶”ê°€
    return setup_lora_training(model)
```

---

## 4. ë°°í¬ ìµœì í™”

### 4.1 ì–‘ìí™”

```python
"""
SLM ì–‘ìí™”: ë©”ëª¨ë¦¬ & ì†ë„ ìµœì í™”
"""

# 1. GPTQ (Post-Training Quantization)
from transformers import GPTQConfig

def quantize_with_gptq(model_name):
    gptq_config = GPTQConfig(
        bits=4,
        dataset="c4",
        tokenizer=AutoTokenizer.from_pretrained(model_name)
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=gptq_config,
        device_map="auto"
    )

    return model


# 2. AWQ (Activation-aware Weight Quantization)
from awq import AutoAWQForCausalLM

def quantize_with_awq(model_path, output_path):
    model = AutoAWQForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # ì–‘ìí™”
    model.quantize(
        tokenizer,
        quant_config={
            "zero_point": True,
            "q_group_size": 128,
            "w_bit": 4,
            "version": "GEMM"
        }
    )

    # ì €ì¥
    model.save_quantized(output_path)


# 3. llama.cpp (GGUF í¬ë§·)
"""
llama.cpp ì–‘ìí™” ë ˆë²¨:
- Q2_K: 2ë¹„íŠ¸ (ë§¤ìš° ì‘ìŒ, í’ˆì§ˆ ì €í•˜)
- Q4_K_M: 4ë¹„íŠ¸ (ê¶Œì¥, í’ˆì§ˆ/í¬ê¸° ê· í˜•)
- Q5_K_M: 5ë¹„íŠ¸ (ë†’ì€ í’ˆì§ˆ)
- Q8_0: 8ë¹„íŠ¸ (ê±°ì˜ ì›ë³¸ í’ˆì§ˆ)

ëª…ë ¹ì–´:
./quantize model.gguf model-q4_k_m.gguf Q4_K_M
"""


# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
def compare_memory_usage():
    """íŒŒë¼ë¯¸í„° ìˆ˜ì— ë”°ë¥¸ ë©”ëª¨ë¦¬"""
    configs = [
        ("3B FP16", 3e9 * 2),       # 6GB
        ("3B Q8", 3e9 * 1),         # 3GB
        ("3B Q4", 3e9 * 0.5),       # 1.5GB
        ("7B FP16", 7e9 * 2),       # 14GB
        ("7B Q4", 7e9 * 0.5),       # 3.5GB
    ]

    print("Model\t\tMemory (GB)")
    print("-" * 30)
    for name, memory in configs:
        print(f"{name}\t\t{memory / 1e9:.1f}")
```

### 4.2 On-Device ë°°í¬

```python
"""
ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤ ë°°í¬
"""

# 1. ONNX ë³€í™˜
def convert_to_onnx(model, tokenizer, output_path):
    from optimum.onnxruntime import ORTModelForCausalLM

    # ONNX ë³€í™˜ ë° ìµœì í™”
    ort_model = ORTModelForCausalLM.from_pretrained(
        model,
        export=True,
        provider="CPUExecutionProvider"
    )

    ort_model.save_pretrained(output_path)


# 2. TensorRT-LLM (NVIDIA GPU)
"""
TensorRT-LLM ì‚¬ìš©:
1. ëª¨ë¸ ë³€í™˜: python convert_checkpoint.py
2. ì—”ì§„ ë¹Œë“œ: trtllm-build
3. ì¶”ë¡ : python run.py
"""


# 3. llama.cpp (CPU ì¶”ë¡ )
"""
llama.cpp ì‚¬ìš©:
1. GGUF ë³€í™˜
2. llama-cli ì‹¤í–‰

./llama-cli -m model.gguf \
    -n 256 \
    -p "Hello, how are you?" \
    -t 4  # threads
"""


# 4. MLC-LLM (ë‹¤ì–‘í•œ í”Œë«í¼)
"""
MLC-LLM: iOS, Android, WebGPU, CUDA

mlc_chat ì•±ìœ¼ë¡œ ëª¨ë°”ì¼ ë°°í¬ ê°€ëŠ¥
"""
```

---

## 5. ë²¤ì¹˜ë§ˆí¬ & í‰ê°€

### 5.1 SLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SLM ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (2024.10 ê¸°ì¤€)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Model          Params  MMLU    GSM8K   HumanEval  TriviaQA     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Phi-3-mini     3.8B    69.9%   82.5%   57.9%      63.5%        â”‚
â”‚  Gemma-2-9B     9B      71.3%   68.6%   54.3%      73.5%        â”‚
â”‚  Qwen2.5-7B     7B      74.2%   82.6%   75.6%      71.4%        â”‚
â”‚  Llama-3.2-3B   3B      63.4%   44.4%   36.0%      63.4%        â”‚
â”‚  SmolLM-1.7B    1.7B    42.3%   18.2%   28.7%      42.1%        â”‚
â”‚                                                                  â”‚
â”‚  ì°¸ê³ : GPT-4    -       86.4%   92.0%   67.0%      87.6%        â”‚
â”‚                                                                  â”‚
â”‚  â€» Phi-3ì€ ì‘ì€ í¬ê¸° ëŒ€ë¹„ ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥                       â”‚
â”‚  â€» Qwen2.5ëŠ” ì½”ë“œ(HumanEval)ì—ì„œ ê°•ì                             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 íƒœìŠ¤í¬ë³„ SLM ì„ íƒ ê°€ì´ë“œ

```python
"""
íƒœìŠ¤í¬ë³„ SLM ì¶”ì²œ
"""

TASK_MODEL_RECOMMENDATIONS = {
    # ì¼ë°˜ ëŒ€í™”
    "general_chat": {
        "best": "Qwen2.5-7B-Instruct",
        "budget": "Qwen2.5-1.5B-Instruct",
        "mobile": "Qwen2.5-0.5B-Instruct"
    },

    # ì½”ë“œ ìƒì„±
    "code_generation": {
        "best": "Qwen2.5-Coder-7B",
        "budget": "CodeGemma-2B",
        "mobile": "Phi-3-mini"
    },

    # ìˆ˜í•™/ì¶”ë¡ 
    "math_reasoning": {
        "best": "Qwen2.5-Math-7B",
        "budget": "Phi-3-mini",
        "mobile": "Phi-3-mini"
    },

    # í•œêµ­ì–´
    "korean": {
        "best": "Qwen2.5-7B-Instruct",  # ë‹¤êµ­ì–´ ê°•ì 
        "budget": "EXAONE-3.0-7.8B-Instruct",
        "mobile": "Qwen2.5-1.5B-Instruct"
    },

    # RAG/ê²€ìƒ‰
    "rag": {
        "best": "Gemma-2-9B",
        "budget": "Llama-3.2-3B",
        "mobile": "Phi-3-mini"
    },

    # ìš”ì•½
    "summarization": {
        "best": "Qwen2.5-7B-Instruct",
        "budget": "Gemma-2-2B",
        "mobile": "SmolLM-1.7B"
    }
}


def select_model(task: str, constraint: str = "best"):
    """íƒœìŠ¤í¬ì™€ ì œì•½ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ"""
    if task in TASK_MODEL_RECOMMENDATIONS:
        return TASK_MODEL_RECOMMENDATIONS[task].get(constraint)
    return "Qwen2.5-7B-Instruct"  # ê¸°ë³¸ê°’
```

---

## 6. ì‹¤ìŠµ: SLM Fine-tuning

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset

def finetune_slm():
    """SLM QLoRA Fine-tuning ì˜ˆì œ"""

    # 1. ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™”)
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. LoRA ì„¤ì •
    model = prepare_model_for_kbit_training(model)

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 3. ë°ì´í„°ì…‹
    dataset = load_dataset("timdettmers/openassistant-guanaco")

    def preprocess(examples):
        texts = []
        for text in examples['text']:
            # Qwen chat format
            texts.append(text + tokenizer.eos_token)

        tokenized = tokenizer(
            texts,
            truncation=True,
            max_length=1024,
            padding="max_length"
        )
        tokenized['labels'] = tokenized['input_ids'].copy()
        return tokenized

    tokenized_dataset = dataset['train'].map(
        preprocess,
        batched=True,
        remove_columns=dataset['train'].column_names
    )

    # 4. í•™ìŠµ
    training_args = TrainingArguments(
        output_dir="./qwen-finetuned",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        logging_steps=10,
        save_steps=500,
        bf16=True,
        optim="paged_adamw_8bit"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )

    trainer.train()

    # 5. ì €ì¥
    model.save_pretrained("./qwen-lora-adapter")

    print("Fine-tuning complete!")


if __name__ == "__main__":
    finetune_slm()
```

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- Gunasekar et al. (2023). "Textbooks Are All You Need" (Phi)
- Gemma Team (2024). "Gemma 2: Improving Open Language Models"
- Yang et al. (2024). "Qwen2 Technical Report"

### ëª¨ë¸
- [Phi-3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
- [Gemma 2](https://huggingface.co/google/gemma-2-9b)
- [Qwen 2.5](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)

### ê´€ë ¨ ë ˆìŠ¨
- [../LLM_and_NLP/11_Model_Quantization.md](../LLM_and_NLP/11_Model_Quantization.md)
- [19_PEFT_Unified.md](19_PEFT_Unified.md)
