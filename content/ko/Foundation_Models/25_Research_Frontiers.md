# 25. Research Frontiers

## 개요

Foundation Model 연구의 최전선을 다룹니다. World Models, o1-style Reasoning, Synthetic Data, Multi-Agent 시스템 등 미래 방향을 탐구합니다.

---

## 1. o1-style Reasoning (Test-time Compute)

### 1.1 개념

```
기존 LLM vs o1-style:
┌─────────────────────────────────────────────────────────┐
│  기존 LLM:                                              │
│  - 학습 시간에 계산 집중 (더 큰 모델, 더 많은 데이터)   │
│  - 추론 시 고정된 forward pass                          │
│  - 복잡한 문제에 한계                                   │
│                                                         │
│  o1-style (Test-time Compute Scaling):                 │
│  - 추론 시 더 많은 계산 사용                            │
│  - Chain-of-Thought 자동 생성                          │
│  - 여러 경로 탐색 후 최선 선택                          │
│  - 문제 난이도에 따라 적응적 계산                       │
└─────────────────────────────────────────────────────────┘

핵심 기법:
1. Internal Chain-of-Thought
2. Search/Verification loops
3. Self-consistency checking
4. Reward model guided search
```

### 1.2 개념적 구현

```python
import torch
from typing import List, Tuple

class ReasoningModel:
    """o1-style 추론 모델 (개념적 구현)"""

    def __init__(self, base_model, reward_model):
        self.model = base_model
        self.reward_model = reward_model

    def reason(
        self,
        problem: str,
        max_thinking_tokens: int = 10000,
        num_candidates: int = 5
    ) -> str:
        """확장된 추론"""
        # 1. 여러 reasoning chain 생성
        candidates = self._generate_candidates(problem, num_candidates)

        # 2. 각 chain 평가
        scored_candidates = []
        for chain, answer in candidates:
            score = self._evaluate_chain(chain, answer)
            scored_candidates.append((chain, answer, score))

        # 3. 최선의 답변 선택
        best = max(scored_candidates, key=lambda x: x[2])
        return best[1]  # 답변만 반환 (chain은 내부)

    def _generate_candidates(
        self,
        problem: str,
        n: int
    ) -> List[Tuple[str, str]]:
        """여러 추론 경로 생성"""
        candidates = []

        for _ in range(n):
            # Step-by-step reasoning 생성
            chain = self._generate_reasoning_chain(problem)

            # Chain에서 최종 답변 추출
            answer = self._extract_answer(chain)

            candidates.append((chain, answer))

        return candidates

    def _generate_reasoning_chain(self, problem: str) -> str:
        """추론 체인 생성"""
        prompt = f"""Solve this problem step by step.
Think carefully and show your reasoning.

Problem: {problem}

Let me think through this carefully..."""

        # 길이 제한 없이 생성 (또는 매우 긴 제한)
        response = self.model.generate(
            prompt,
            max_new_tokens=5000,
            temperature=0.7
        )

        return response

    def _evaluate_chain(self, chain: str, answer: str) -> float:
        """추론 체인 품질 평가"""
        # Reward model로 평가
        score = self.reward_model.evaluate(chain)

        # 자기 일관성 체크
        consistency_score = self._check_consistency(chain, answer)

        return score * 0.7 + consistency_score * 0.3

    def _check_consistency(self, chain: str, answer: str) -> float:
        """논리적 일관성 검사"""
        # 간단한 휴리스틱 또는 별도 모델 사용
        prompt = f"""Is this reasoning chain logically consistent?

Reasoning:
{chain}

Answer: {answer}

Rate consistency (0-1):"""

        response = self.model.generate(prompt, max_new_tokens=10)
        # 파싱...
        return 0.8  # 예시


class TreeOfThoughts:
    """Tree of Thoughts 구현"""

    def __init__(self, model, evaluator):
        self.model = model
        self.evaluator = evaluator

    def solve(
        self,
        problem: str,
        depth: int = 3,
        branching_factor: int = 3
    ) -> str:
        """트리 탐색으로 문제 해결"""
        root = {"state": problem, "thoughts": [], "score": 0}
        best_path = self._search(root, depth, branching_factor)
        return self._extract_solution(best_path)

    def _search(self, node: dict, depth: int, bf: int) -> List[dict]:
        """BFS/DFS 탐색"""
        if depth == 0:
            return [node]

        # 다음 단계 생각들 생성
        thoughts = self._generate_thoughts(node, bf)

        # 각 생각 평가
        children = []
        for thought in thoughts:
            child = {
                "state": node["state"],
                "thoughts": node["thoughts"] + [thought],
                "score": self._evaluate_thought(thought, node)
            }
            children.append(child)

        # 상위 b개만 확장 (beam search)
        children.sort(key=lambda x: x["score"], reverse=True)
        children = children[:bf]

        # 재귀 탐색
        best_paths = []
        for child in children:
            path = self._search(child, depth - 1, bf)
            best_paths.extend(path)

        return sorted(best_paths, key=lambda x: x["score"], reverse=True)[:1]

    def _generate_thoughts(self, node: dict, n: int) -> List[str]:
        """다음 단계 생각 생성"""
        context = "\n".join(node["thoughts"])

        prompt = f"""Problem: {node["state"]}

Previous thoughts:
{context}

Generate {n} different next steps or approaches:"""

        response = self.model.generate(prompt)
        # 파싱하여 n개 생각 추출
        return response.split("\n")[:n]

    def _evaluate_thought(self, thought: str, node: dict) -> float:
        """생각의 품질 평가"""
        return self.evaluator.score(thought, node["state"])
```

---

## 2. Synthetic Data

### 2.1 개념

```
Synthetic Data Generation:
┌─────────────────────────────────────────────────────────┐
│  문제: 고품질 학습 데이터 부족                           │
│                                                         │
│  해결: LLM으로 학습 데이터 생성                         │
│                                                         │
│  방법:                                                  │
│  1. Self-Instruct: instruction/response 쌍 생성        │
│  2. Evol-Instruct: 점진적 복잡화                        │
│  3. Rejection Sampling: 다수 생성 후 필터링             │
│  4. RLHF-style: 선호도 데이터 생성                      │
│  5. Distillation: 강한 모델에서 약한 모델로             │
│                                                         │
│  주의:                                                  │
│  - Model collapse (자기 데이터로만 학습 시)             │
│  - 다양성 유지 중요                                     │
│  - 품질 검증 필수                                       │
└─────────────────────────────────────────────────────────┘
```

### 2.2 구현

```python
class SyntheticDataGenerator:
    """합성 데이터 생성기"""

    def __init__(self, teacher_model, student_model=None):
        self.teacher = teacher_model
        self.student = student_model

    def generate_instruction_data(
        self,
        seed_instructions: List[str],
        num_samples: int = 10000,
        diversity_threshold: float = 0.7
    ) -> List[dict]:
        """Instruction-Response 데이터 생성"""
        generated = []
        instruction_embeddings = []

        while len(generated) < num_samples:
            # 새 instruction 생성
            instruction = self._generate_instruction(seed_instructions + [
                g["instruction"] for g in generated[-10:]
            ])

            # 다양성 체크
            if self._check_diversity(instruction, instruction_embeddings, diversity_threshold):
                # Response 생성
                response = self._generate_response(instruction)

                # 품질 체크
                if self._quality_check(instruction, response):
                    generated.append({
                        "instruction": instruction,
                        "response": response
                    })

                    # 임베딩 저장
                    emb = self._get_embedding(instruction)
                    instruction_embeddings.append(emb)

            if len(generated) % 100 == 0:
                print(f"Generated {len(generated)}/{num_samples}")

        return generated

    def _generate_instruction(self, examples: List[str]) -> str:
        """새 instruction 생성"""
        examples_text = "\n".join([f"- {ex}" for ex in examples[-5:]])

        prompt = f"""Here are some example instructions:
{examples_text}

Generate a new, different instruction that is:
1. Clear and specific
2. Different from the examples
3. Useful and educational

New instruction:"""

        return self.teacher.generate(prompt, temperature=0.9)

    def _generate_response(self, instruction: str) -> str:
        """Response 생성"""
        prompt = f"""Instruction: {instruction}

Please provide a helpful, accurate, and detailed response:"""

        return self.teacher.generate(prompt, temperature=0.7)

    def _check_diversity(
        self,
        instruction: str,
        existing_embeddings: List,
        threshold: float
    ) -> bool:
        """다양성 검사"""
        if not existing_embeddings:
            return True

        new_emb = self._get_embedding(instruction)

        for emb in existing_embeddings:
            similarity = self._cosine_similarity(new_emb, emb)
            if similarity > threshold:
                return False

        return True

    def _quality_check(self, instruction: str, response: str) -> bool:
        """품질 검사"""
        # 길이 체크
        if len(response) < 50:
            return False

        # 관련성 체크 (간단한 휴리스틱)
        instruction_words = set(instruction.lower().split())
        response_words = set(response.lower().split())

        overlap = len(instruction_words & response_words)
        if overlap < 2:
            return False

        return True


class RejectSampling:
    """Rejection Sampling으로 고품질 데이터 선별"""

    def __init__(self, generator_model, reward_model):
        self.generator = generator_model
        self.reward = reward_model

    def generate_with_rejection(
        self,
        prompt: str,
        n_samples: int = 16,
        top_k: int = 1
    ) -> List[str]:
        """다수 생성 후 최선 선택"""
        # 여러 응답 생성
        responses = []
        for _ in range(n_samples):
            response = self.generator.generate(prompt, temperature=0.8)
            responses.append(response)

        # 각 응답 점수화
        scored = []
        for response in responses:
            score = self.reward.score(prompt, response)
            scored.append((response, score))

        # 상위 k개 선택
        scored.sort(key=lambda x: x[1], reverse=True)
        return [r for r, s in scored[:top_k]]
```

---

## 3. Multi-Agent Systems

### 3.1 개념

```
Multi-Agent LLM Systems:
┌─────────────────────────────────────────────────────────┐
│  Agent 유형:                                            │
│                                                         │
│  1. Debate: 여러 Agent가 토론                           │
│     - 서로 다른 관점 제시                               │
│     - 합의 도출                                         │
│                                                         │
│  2. Collaboration: 역할 분담 협업                       │
│     - 작성자, 검토자, 편집자                            │
│     - 연구자, 개발자, 테스터                            │
│                                                         │
│  3. Competition: 경쟁적 생성                            │
│     - 최선의 결과 선택                                  │
│     - Red team / Blue team                              │
│                                                         │
│  4. Hierarchical: 계층적 구조                           │
│     - Manager → Worker agents                           │
│     - 태스크 분해 및 위임                               │
└─────────────────────────────────────────────────────────┘
```

### 3.2 구현

```python
from typing import Dict, Any
from dataclasses import dataclass
from enum import Enum

class AgentRole(Enum):
    PLANNER = "planner"
    RESEARCHER = "researcher"
    WRITER = "writer"
    CRITIC = "critic"
    EDITOR = "editor"

@dataclass
class Message:
    sender: str
    receiver: str
    content: str
    metadata: Dict[str, Any] = None

class MultiAgentSystem:
    """다중 에이전트 시스템"""

    def __init__(self, llm):
        self.llm = llm
        self.agents = {}
        self.message_history = []

    def add_agent(self, name: str, role: AgentRole, system_prompt: str):
        """에이전트 추가"""
        self.agents[name] = {
            "role": role,
            "system_prompt": system_prompt,
            "memory": []
        }

    def send_message(self, sender: str, receiver: str, content: str):
        """메시지 전송"""
        message = Message(sender=sender, receiver=receiver, content=content)
        self.message_history.append(message)
        self.agents[receiver]["memory"].append(message)

        return self._get_response(receiver)

    def _get_response(self, agent_name: str) -> str:
        """에이전트 응답 생성"""
        agent = self.agents[agent_name]

        # 최근 메시지로 컨텍스트 구성
        recent_messages = agent["memory"][-5:]
        context = "\n".join([
            f"{m.sender}: {m.content}" for m in recent_messages
        ])

        prompt = f"""{agent["system_prompt"]}

Recent conversation:
{context}

Your response as {agent_name}:"""

        return self.llm.generate(prompt)

    def run_debate(
        self,
        topic: str,
        agents: List[str],
        rounds: int = 3
    ) -> str:
        """토론 실행"""
        # 초기 의견
        opinions = {}
        for agent in agents:
            response = self.send_message(
                "moderator", agent,
                f"What is your position on: {topic}"
            )
            opinions[agent] = response

        # 토론 라운드
        for round in range(rounds):
            for agent in agents:
                # 다른 에이전트 의견 전달
                other_opinions = "\n".join([
                    f"{a}: {o}" for a, o in opinions.items() if a != agent
                ])

                response = self.send_message(
                    "moderator", agent,
                    f"Others' opinions:\n{other_opinions}\n\nYour response:"
                )
                opinions[agent] = response

        # 합의 도출
        final_opinions = "\n".join([f"{a}: {o}" for a, o in opinions.items()])
        consensus = self.llm.generate(
            f"Based on this debate, summarize the consensus:\n{final_opinions}"
        )

        return consensus


class CollaborativeWriting:
    """협업 글쓰기 시스템"""

    def __init__(self, llm):
        self.system = MultiAgentSystem(llm)

        # 에이전트 설정
        self.system.add_agent(
            "writer",
            AgentRole.WRITER,
            "You are a creative writer. Write engaging content."
        )
        self.system.add_agent(
            "critic",
            AgentRole.CRITIC,
            "You are a critical reviewer. Point out issues and suggest improvements."
        )
        self.system.add_agent(
            "editor",
            AgentRole.EDITOR,
            "You are an editor. Refine and polish the writing."
        )

    def write(self, topic: str, iterations: int = 3) -> str:
        """협업 글쓰기"""
        # 초안 작성
        draft = self.system.send_message(
            "user", "writer",
            f"Write a short article about: {topic}"
        )

        for i in range(iterations):
            # 비평
            critique = self.system.send_message(
                "writer", "critic",
                f"Please review this draft:\n{draft}"
            )

            # 수정
            revised = self.system.send_message(
                "critic", "writer",
                f"Based on this feedback:\n{critique}\n\nPlease revise the draft."
            )

            draft = revised

        # 최종 편집
        final = self.system.send_message(
            "writer", "editor",
            f"Please polish this final draft:\n{draft}"
        )

        return final
```

---

## 4. World Models

### 4.1 개념

```
World Models:
┌─────────────────────────────────────────────────────────┐
│  목표: LLM이 세계의 동작 방식을 이해하고 시뮬레이션     │
│                                                         │
│  응용:                                                  │
│  1. Planning: 행동의 결과 예측                          │
│  2. Reasoning: 인과 관계 추론                           │
│  3. Simulation: 가상 환경 시뮬레이션                    │
│  4. Embodied AI: 로봇 제어                              │
│                                                         │
│  연구 방향:                                             │
│  - Video generation as world simulation (Sora)          │
│  - Physical reasoning benchmarks                        │
│  - Embodied language models                             │
│  - Causal reasoning                                     │
└─────────────────────────────────────────────────────────┘
```

### 4.2 개념적 구현

```python
class WorldModel:
    """World Model 개념적 구현"""

    def __init__(self, llm):
        self.llm = llm
        self.state = {}

    def initialize_state(self, description: str):
        """초기 상태 설정"""
        prompt = f"""Parse this scene description into structured state.

Description: {description}

Extract:
- Objects (name, position, properties)
- Relationships between objects
- Physical constraints

State:"""

        state_text = self.llm.generate(prompt)
        self.state = self._parse_state(state_text)

    def predict_action_result(self, action: str) -> Dict:
        """행동 결과 예측"""
        state_description = self._describe_state()

        prompt = f"""Current state:
{state_description}

Action: {action}

Predict:
1. What changes will occur?
2. What is the new state?
3. Any unexpected effects?

Prediction:"""

        prediction = self.llm.generate(prompt)
        return self._parse_prediction(prediction)

    def simulate_sequence(
        self,
        actions: List[str]
    ) -> List[Dict]:
        """행동 시퀀스 시뮬레이션"""
        states = [self.state.copy()]

        for action in actions:
            prediction = self.predict_action_result(action)
            self._apply_changes(prediction)
            states.append(self.state.copy())

        return states

    def _describe_state(self) -> str:
        """상태를 텍스트로 설명"""
        # state dict를 자연어로 변환
        return str(self.state)

    def _parse_state(self, text: str) -> Dict:
        """텍스트를 상태로 파싱"""
        # 실제로는 더 정교한 파싱 필요
        return {"raw": text}

    def _parse_prediction(self, text: str) -> Dict:
        """예측 결과 파싱"""
        return {"raw": text}

    def _apply_changes(self, prediction: Dict):
        """예측된 변화 적용"""
        # 상태 업데이트
        pass
```

---

## 5. 미래 연구 방향

### 5.1 주요 방향

```
1. Scaling Laws Beyond Parameters
   - Test-time compute scaling
   - Mixture of Experts scaling
   - Data quality over quantity

2. Multimodal Understanding
   - Native multimodal models
   - Embodied AI
   - Physical world understanding

3. Reasoning Enhancement
   - Formal verification
   - Neuro-symbolic integration
   - Causal reasoning

4. Alignment & Safety
   - Constitutional AI
   - Interpretability
   - Robustness to adversarial inputs

5. Efficiency
   - Sparse architectures
   - Mixture of Depths
   - Early exit mechanisms
```

### 5.2 열린 문제들

```
┌─────────────────────────────────────────────────────────┐
│  열린 연구 문제:                                        │
│                                                         │
│  1. Hallucination 완전 해결                             │
│     - 언제 모르는지 아는 것                             │
│     - 신뢰도 calibration                                │
│                                                         │
│  2. True Reasoning vs Pattern Matching                  │
│     - 진정한 일반화 능력?                               │
│     - Out-of-distribution 추론                          │
│                                                         │
│  3. Long-term Memory                                    │
│     - 영구적 학습                                       │
│     - Continual learning without forgetting             │
│                                                         │
│  4. Efficiency-Capability Tradeoff                      │
│     - 작은 모델의 한계?                                 │
│     - Knowledge distillation 한계                       │
│                                                         │
│  5. Alignment                                           │
│     - Value alignment의 정의                            │
│     - Scalable oversight                                │
└─────────────────────────────────────────────────────────┘
```

---

## 핵심 정리

### Research Frontiers 요약
```
1. o1-style: 추론 시 더 많은 계산
2. Synthetic Data: LLM으로 학습 데이터 생성
3. Multi-Agent: 협업/토론/경쟁 시스템
4. World Models: 물리 세계 시뮬레이션
5. Alignment: 안전하고 유용한 AI
```

### 미래 전망
```
- Parameter scaling → Compute scaling
- Single model → Multi-agent systems
- Text → Native multimodal
- Pattern matching → True reasoning
- Black box → Interpretable
```

---

## 참고 자료

1. OpenAI (2024). "Learning to Reason with LLMs" (o1)
2. Yao et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
3. Park et al. (2023). "Generative Agents: Interactive Simulacra of Human Behavior"
4. Ha & Schmidhuber (2018). "World Models"
5. Sora Technical Report (2024)
