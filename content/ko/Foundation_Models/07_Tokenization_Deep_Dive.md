# 07. Tokenization ì‹¬í™”

## ê°œìš”

Tokenizationì€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. Foundation Modelì˜ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

---

## 1. Tokenization íŒ¨ëŸ¬ë‹¤ì„

### 1.1 ì—­ì‚¬ì  ë°œì „

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Tokenization ì§„í™”                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Word-level (ì „í†µ)                                               â”‚
â”‚  "I love NLP" â†’ ["I", "love", "NLP"]                            â”‚
â”‚  ë¬¸ì œ: OOV (Out-of-Vocabulary), ê±°ëŒ€í•œ ì–´íœ˜ í¬ê¸°                 â”‚
â”‚                                                                  â”‚
â”‚       â†“                                                          â”‚
â”‚                                                                  â”‚
â”‚  Character-level                                                 â”‚
â”‚  "I love NLP" â†’ ["I", " ", "l", "o", "v", "e", " ", ...]        â”‚
â”‚  ë¬¸ì œ: ë„ˆë¬´ ê¸´ ì‹œí€€ìŠ¤, ì˜ë¯¸ ë‹¨ìœ„ ì†ì‹¤                            â”‚
â”‚                                                                  â”‚
â”‚       â†“                                                          â”‚
â”‚                                                                  â”‚
â”‚  Subword (í˜„ì¬ ì£¼ë¥˜)                                             â”‚
â”‚  "I love NLP" â†’ ["I", "Ä love", "Ä N", "LP"]                      â”‚
â”‚  ì¥ì : OOV ì—†ìŒ, ì ì ˆí•œ ì‹œí€€ìŠ¤ ê¸¸ì´, í˜•íƒœì†Œì  ì˜ë¯¸ ë³´ì¡´          â”‚
â”‚                                                                  â”‚
â”‚       â†“ (ë¯¸ë˜)                                                   â”‚
â”‚                                                                  â”‚
â”‚  Byte-level / Tokenizer-free                                    â”‚
â”‚  Raw bytes ë˜ëŠ” í•™ìŠµëœ í† í°í™” ì—†ì´ ì²˜ë¦¬                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | ë°©ì‹ | ëŒ€í‘œ ëª¨ë¸ | íŠ¹ì§• |
|----------|------|-----------|------|
| **BPE** | ë¹ˆë„ ê¸°ë°˜ ë³‘í•© | GPT, RoBERTa, LLaMA | ê°€ì¥ ë„ë¦¬ ì‚¬ìš© |
| **WordPiece** | ìš°ë„ ê¸°ë°˜ ë³‘í•© | BERT, DistilBERT | í™•ë¥ ì  ì„ íƒ |
| **Unigram** | í™•ë¥  ëª¨ë¸ | T5, ALBERT, XLNet | ìµœì  ë¶„í•  íƒìƒ‰ |
| **SentencePiece** | ì–¸ì–´ ë…ë¦½ì  | ë‹¤êµ­ì–´ ëª¨ë¸ | BPE/Unigram êµ¬í˜„ |

---

## 2. BPE (Byte-Pair Encoding)

### 2.1 ì•Œê³ ë¦¬ì¦˜

```
BPE í•™ìŠµ ê³¼ì •:

1. ì´ˆê¸° ì–´íœ˜ = ëª¨ë“  ë¬¸ì + íŠ¹ìˆ˜ í† í°
2. ë°˜ë³µ:
   a. ê°€ì¥ ë¹ˆë²ˆí•œ ì¸ì ‘ í† í° ìŒ ì°¾ê¸°
   b. í•´ë‹¹ ìŒì„ ìƒˆ í† í°ìœ¼ë¡œ ë³‘í•©
   c. ì–´íœ˜ì— ì¶”ê°€
3. ëª©í‘œ ì–´íœ˜ í¬ê¸°ê¹Œì§€ ë°˜ë³µ

ì˜ˆì‹œ:
ì´ˆê¸°: ['l', 'o', 'w', 'e', 'r', 'n', 'i', 'g', 'h', 't']

Step 1: 'l' + 'o' â†’ 'lo' (ê°€ì¥ ë¹ˆë²ˆ)
Step 2: 'lo' + 'w' â†’ 'low'
Step 3: 'e' + 'r' â†’ 'er'
Step 4: 'n' + 'i' â†’ 'ni'
Step 5: 'ni' + 'g' â†’ 'nig'
Step 6: 'nig' + 'h' â†’ 'nigh'
Step 7: 'nigh' + 't' â†’ 'night'
...

ìµœì¢…: "lower" â†’ ['low', 'er'], "night" â†’ ['night']
```

### 2.2 êµ¬í˜„

```python
from collections import Counter, defaultdict
from typing import Dict, List, Tuple
import re

class BPETokenizer:
    """Byte-Pair Encoding Tokenizer"""

    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.merges = {}
        self.special_tokens = ['<pad>', '<unk>', '<bos>', '<eos>']

    def train(self, texts: List[str]):
        """BPE í•™ìŠµ"""
        # 1. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freqs = self._count_words(texts)

        # 2. ì´ˆê¸° ì–´íœ˜ (ë¬¸ì ë‹¨ìœ„)
        self.vocab = {char: i for i, char in enumerate(self.special_tokens)}
        for word in word_freqs:
            for char in word:
                if char not in self.vocab:
                    self.vocab[char] = len(self.vocab)

        # 3. ë‹¨ì–´ë¥¼ ë¬¸ì ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• 
        splits = {word: list(word) for word in word_freqs}

        # 4. ë³‘í•© ë°˜ë³µ
        while len(self.vocab) < self.vocab_size:
            # ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ì°¾ê¸°
            pair_freqs = self._count_pairs(splits, word_freqs)
            if not pair_freqs:
                break

            best_pair = max(pair_freqs, key=pair_freqs.get)

            # ë³‘í•©
            splits = self._merge_pair(splits, best_pair)

            # ì–´íœ˜ì— ì¶”ê°€
            new_token = ''.join(best_pair)
            self.vocab[new_token] = len(self.vocab)
            self.merges[best_pair] = new_token

            if len(self.vocab) % 1000 == 0:
                print(f"Vocab size: {len(self.vocab)}")

    def _count_words(self, texts: List[str]) -> Dict[str, int]:
        """ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°"""
        word_freqs = Counter()
        for text in texts:
            words = text.split()
            word_freqs.update(words)
        return dict(word_freqs)

    def _count_pairs(
        self,
        splits: Dict[str, List[str]],
        word_freqs: Dict[str, int]
    ) -> Dict[Tuple[str, str], int]:
        """ì¸ì ‘ í† í° ìŒ ë¹ˆë„ ê³„ì‚°"""
        pair_freqs = defaultdict(int)
        for word, freq in word_freqs.items():
            split = splits[word]
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq
        return pair_freqs

    def _merge_pair(
        self,
        splits: Dict[str, List[str]],
        pair: Tuple[str, str]
    ) -> Dict[str, List[str]]:
        """ìŒì„ ë³‘í•©"""
        new_splits = {}
        for word, split in splits.items():
            new_split = []
            i = 0
            while i < len(split):
                if i < len(split) - 1 and (split[i], split[i + 1]) == pair:
                    new_split.append(split[i] + split[i + 1])
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            new_splits[word] = new_split
        return new_splits

    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ â†’ í† í° ID"""
        words = text.split()
        ids = []

        for word in words:
            # ë¬¸ìë¡œ ë¶„í• 
            tokens = list(word)

            # í•™ìŠµëœ ë³‘í•© ì ìš©
            for pair, merged in self.merges.items():
                i = 0
                while i < len(tokens) - 1:
                    if (tokens[i], tokens[i + 1]) == pair:
                        tokens = tokens[:i] + [merged] + tokens[i + 2:]
                    else:
                        i += 1

            # IDë¡œ ë³€í™˜
            for token in tokens:
                if token in self.vocab:
                    ids.append(self.vocab[token])
                else:
                    ids.append(self.vocab['<unk>'])

        return ids

    def decode(self, ids: List[int]) -> str:
        """í† í° ID â†’ í…ìŠ¤íŠ¸"""
        id_to_token = {v: k for k, v in self.vocab.items()}
        tokens = [id_to_token.get(id, '<unk>') for id in ids]
        return ''.join(tokens)


# ì‚¬ìš© ì˜ˆì‹œ
tokenizer = BPETokenizer(vocab_size=5000)

texts = [
    "the quick brown fox jumps over the lazy dog",
    "machine learning is transforming the world",
    # ... ë” ë§ì€ í…ìŠ¤íŠ¸
]

tokenizer.train(texts * 1000)  # ë°˜ë³µí•˜ì—¬ ì¶©ë¶„í•œ ë¹ˆë„ í™•ë³´

text = "the transformer model"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print(f"Original: {text}")
print(f"IDs: {ids}")
print(f"Decoded: {decoded}")
```

---

## 3. WordPiece

### 3.1 BPEì™€ì˜ ì°¨ì´ì 

```
BPE: ë¹ˆë„ ê¸°ë°˜
- ê°€ì¥ ë¹ˆë²ˆí•œ ìŒì„ ë³‘í•©
- count(ab)ê°€ ìµœëŒ€ì¸ (a, b) ì„ íƒ

WordPiece: ìš°ë„ ê¸°ë°˜
- ë³‘í•© ì‹œ ì „ì²´ ìš°ë„ ì¦ê°€ê°€ ìµœëŒ€ì¸ ìŒ ì„ íƒ
- score(a, b) = count(ab) / (count(a) * count(b))
- í¬ê·€ ìŒì´ë”ë¼ë„ êµ¬ì„± ìš”ì†Œê°€ í¬ê·€í•˜ë©´ ì„ íƒë  ìˆ˜ ìˆìŒ
```

### 3.2 êµ¬í˜„

```python
class WordPieceTokenizer:
    """WordPiece Tokenizer (BERT ìŠ¤íƒ€ì¼)"""

    def __init__(self, vocab_size: int = 30000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.prefix = "##"  # ë‹¨ì–´ ë‚´ë¶€ í† í° í‘œì‹œ

    def train(self, texts: List[str]):
        """WordPiece í•™ìŠµ"""
        word_freqs = self._count_words(texts)

        # ì´ˆê¸° ì–´íœ˜: ë¬¸ì + ## ì ‘ë‘ì‚¬ ë²„ì „
        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}

        chars = set()
        for word in word_freqs:
            for i, char in enumerate(word):
                if i == 0:
                    chars.add(char)
                else:
                    chars.add(self.prefix + char)

        for char in sorted(chars):
            self.vocab[char] = len(self.vocab)

        # ë¶„í•  ì´ˆê¸°í™”
        splits = {}
        for word in word_freqs:
            split = [word[0]] + [self.prefix + c for c in word[1:]]
            splits[word] = split

        # ë³‘í•© (ìš°ë„ ê¸°ë°˜)
        while len(self.vocab) < self.vocab_size:
            pair_scores = self._compute_pair_scores(splits, word_freqs)
            if not pair_scores:
                break

            best_pair = max(pair_scores, key=pair_scores.get)
            splits = self._merge_pair(splits, best_pair)

            new_token = best_pair[0] + best_pair[1].replace(self.prefix, '')
            self.vocab[new_token] = len(self.vocab)

    def _compute_pair_scores(
        self,
        splits: Dict[str, List[str]],
        word_freqs: Dict[str, int]
    ) -> Dict[Tuple[str, str], float]:
        """WordPiece ì ìˆ˜ ê³„ì‚°"""
        # ê°œë³„ í† í° ë¹ˆë„
        token_freqs = defaultdict(int)
        for word, freq in word_freqs.items():
            for token in splits[word]:
                token_freqs[token] += freq

        # ìŒ ë¹ˆë„
        pair_freqs = defaultdict(int)
        for word, freq in word_freqs.items():
            split = splits[word]
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq

        # ì ìˆ˜: count(ab) / (count(a) * count(b))
        scores = {}
        for pair, freq in pair_freqs.items():
            score = freq / (token_freqs[pair[0]] * token_freqs[pair[1]])
            scores[pair] = score

        return scores

    def _merge_pair(
        self,
        splits: Dict[str, List[str]],
        pair: Tuple[str, str]
    ) -> Dict[str, List[str]]:
        """ìŒ ë³‘í•©"""
        new_splits = {}
        merged = pair[0] + pair[1].replace(self.prefix, '')

        for word, split in splits.items():
            new_split = []
            i = 0
            while i < len(split):
                if i < len(split) - 1 and (split[i], split[i + 1]) == pair:
                    new_split.append(merged)
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            new_splits[word] = new_split

        return new_splits

    def encode(self, text: str) -> List[int]:
        """Greedy longest-match tokenization"""
        words = text.lower().split()
        ids = []

        for word in words:
            tokens = self._tokenize_word(word)
            for token in tokens:
                ids.append(self.vocab.get(token, self.vocab['[UNK]']))

        return ids

    def _tokenize_word(self, word: str) -> List[str]:
        """ë‹¨ì–´ë¥¼ WordPiece í† í°ìœ¼ë¡œ ë¶„í• """
        tokens = []
        start = 0

        while start < len(word):
            end = len(word)
            found = False

            while start < end:
                substr = word[start:end]
                if start > 0:
                    substr = self.prefix + substr

                if substr in self.vocab:
                    tokens.append(substr)
                    found = True
                    break

                end -= 1

            if not found:
                tokens.append('[UNK]')
                start += 1
            else:
                start = end

        return tokens
```

---

## 4. Unigram LM

### 4.1 ê°œë…

```
Unigram: í™•ë¥ ì  í† í°í™”

1. í° ì´ˆê¸° ì–´íœ˜ë¡œ ì‹œì‘ (substrings)
2. ê° í† í°ì˜ í™•ë¥  ì¶”ì •: P(token)
3. Viterbi ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì  ë¶„í• :
   argmax P(x_1) * P(x_2) * ... * P(x_n)
4. ì–´íœ˜ ì¶•ì†Œ: ì œê±° ì‹œ ì†ì‹¤ì´ ì‘ì€ í† í° ì œê±°
5. ëª©í‘œ í¬ê¸°ê¹Œì§€ ë°˜ë³µ

ì¥ì :
- BPE/WordPieceì™€ ë‹¬ë¦¬ ì—¬ëŸ¬ ë¶„í•  í›„ë³´ ìƒ˜í”Œë§ ê°€ëŠ¥
- ë” robustí•œ í† í°í™”
```

### 4.2 SentencePieceì™€ í•¨ê»˜ ì‚¬ìš©

```python
import sentencepiece as spm

# SentencePiece í•™ìŠµ (BPE ë˜ëŠ” Unigram)
def train_sentencepiece(
    input_file: str,
    model_prefix: str,
    vocab_size: int = 32000,
    model_type: str = 'unigram'  # 'bpe' or 'unigram'
):
    """SentencePiece ëª¨ë¸ í•™ìŠµ"""
    spm.SentencePieceTrainer.train(
        input=input_file,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type=model_type,
        character_coverage=0.9995,  # ë‹¤êµ­ì–´ìš©
        num_threads=16,
        split_digits=True,  # ìˆ«ì ë¶„ë¦¬
        byte_fallback=True,  # OOVë¥¼ byteë¡œ ì²˜ë¦¬
        # íŠ¹ìˆ˜ í† í°
        pad_id=0,
        unk_id=1,
        bos_id=2,
        eos_id=3,
        pad_piece='<pad>',
        unk_piece='<unk>',
        bos_piece='<s>',
        eos_piece='</s>',
    )


# ì‚¬ìš©
def use_sentencepiece(model_path: str):
    """SentencePiece ì‚¬ìš©"""
    sp = spm.SentencePieceProcessor()
    sp.load(model_path)

    text = "Hello, how are you doing today?"

    # ì¸ì½”ë”©
    ids = sp.encode(text, out_type=int)
    pieces = sp.encode(text, out_type=str)

    print(f"Text: {text}")
    print(f"Pieces: {pieces}")
    print(f"IDs: {ids}")

    # ë””ì½”ë”©
    decoded = sp.decode(ids)
    print(f"Decoded: {decoded}")

    # í™•ë¥ ì  ìƒ˜í”Œë§ (Unigramë§Œ)
    for _ in range(3):
        sampled = sp.encode(text, out_type=str, enable_sampling=True, alpha=0.1)
        print(f"Sampled: {sampled}")


# í•™ìŠµ ì˜ˆì‹œ
# train_sentencepiece('corpus.txt', 'tokenizer', vocab_size=32000, model_type='unigram')
# use_sentencepiece('tokenizer.model')
```

---

## 5. Byte-Level BPE

### 5.1 GPT-2/3/4 ìŠ¤íƒ€ì¼

```
Byte-Level BPE:
- ê¸°ë³¸ ì–´íœ˜ = 256 ë°”ì´íŠ¸
- ì–´ë–¤ UTF-8 í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬ ê°€ëŠ¥ (OOV ì—†ìŒ)
- GPT-2ë¶€í„° ì‚¬ìš©

íŠ¹ìˆ˜ ì²˜ë¦¬:
- ê³µë°±: 'Ä ' (G with dot above)ë¡œ í‘œì‹œ
- "Hello world" â†’ ["Hello", "Ä world"]
- ë‹¨ì–´ ê²½ê³„ ëª…ì‹œì  í‘œí˜„
```

### 5.2 HuggingFace Tokenizers

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders
from tokenizers.processors import TemplateProcessing

def create_byte_level_bpe(
    files: List[str],
    vocab_size: int = 50257
) -> Tokenizer:
    """GPT-2 ìŠ¤íƒ€ì¼ Byte-Level BPE ìƒì„±"""

    # 1. ë¹ˆ í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = Tokenizer(models.BPE())

    # 2. Pre-tokenization (ë°”ì´íŠ¸ ë ˆë²¨)
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)

    # 3. ë””ì½”ë”
    tokenizer.decoder = decoders.ByteLevel()

    # 4. í•™ìŠµ
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=['<|endoftext|>', '<|padding|>'],
        show_progress=True,
    )

    tokenizer.train(files, trainer)

    # 5. Post-processing
    tokenizer.post_processor = TemplateProcessing(
        single="$A <|endoftext|>",
        special_tokens=[("<|endoftext|>", tokenizer.token_to_id("<|endoftext|>"))],
    )

    return tokenizer


# ì‚¬ìš©
def demonstrate_byte_level():
    """Byte-Level BPE ë°ëª¨"""
    from transformers import GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    texts = [
        "Hello, world!",
        "ì•ˆë…•í•˜ì„¸ìš”",  # í•œêµ­ì–´
        "ğŸ‰ Party time!",  # ì´ëª¨ì§€
        "The cafÃ© serves naÃ¯ve croissants",  # íŠ¹ìˆ˜ë¬¸ì
    ]

    for text in texts:
        tokens = tokenizer.tokenize(text)
        ids = tokenizer.encode(text)

        print(f"\nText: {text}")
        print(f"Tokens: {tokens}")
        print(f"IDs: {ids}")
        print(f"Decoded: {tokenizer.decode(ids)}")


demonstrate_byte_level()
```

---

## 6. ë‹¤êµ­ì–´ Tokenization

### 6.1 ë„ì „ ê³¼ì œ

```
ë¬¸ì œì :
1. Fertility ë¶ˆê· í˜•: ê°™ì€ ì˜ë¯¸ë¼ë„ ì–¸ì–´ë³„ í† í° ìˆ˜ ì°¨ì´
   - "hello" (1 token) vs "ä½ å¥½" (2-3 tokens) vs "ì•ˆë…•" (2-4 tokens)

2. ì €ìì› ì–¸ì–´ under-representation:
   - ì˜ì–´ ì¤‘ì‹¬ í•™ìŠµ â†’ ë‹¤ë¥¸ ì–¸ì–´ ì–´íœ˜ ë¶€ì¡±

3. ì½”ë“œ ìŠ¤ìœ„ì¹­:
   - "I love ê¹€ì¹˜" â†’ ì˜ì–´/í•œêµ­ì–´ í˜¼ìš© ì²˜ë¦¬

í•´ê²°ì±…:
1. Character coverage: 99.95% ì´ìƒ
2. ì–¸ì–´ë³„ ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì •
3. Byte fallback í™œì„±í™”
```

### 6.2 ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € êµ¬ì¶•

```python
from collections import defaultdict
import unicodedata

class MultilingualTokenizerConfig:
    """ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € ì„¤ì •"""

    # ì–¸ì–´ë³„ ìƒ˜í”Œë§ ë¹„ìœ¨ (BLOOM ìŠ¤íƒ€ì¼)
    LANGUAGE_WEIGHTS = {
        'en': 0.30,   # ì˜ì–´
        'zh': 0.15,   # ì¤‘êµ­ì–´
        'code': 0.15, # í”„ë¡œê·¸ë˜ë° ì½”ë“œ
        'fr': 0.08,   # í”„ë‘ìŠ¤ì–´
        'es': 0.07,   # ìŠ¤í˜ì¸ì–´
        'pt': 0.05,   # í¬ë¥´íˆ¬ê°ˆì–´
        'de': 0.05,   # ë…ì¼ì–´
        'ar': 0.05,   # ì•„ëì–´
        'hi': 0.03,   # íŒë””ì–´
        'ko': 0.02,   # í•œêµ­ì–´
        'ja': 0.02,   # ì¼ë³¸ì–´
        'other': 0.03,
    }

    @staticmethod
    def estimate_fertility(tokenizer, texts_by_lang: dict) -> dict:
        """
        ì–¸ì–´ë³„ Fertility ì¸¡ì •

        Fertility = í† í° ìˆ˜ / ë¬¸ì ìˆ˜
        ë‚®ì„ìˆ˜ë¡ íš¨ìœ¨ì 
        """
        fertility = {}

        for lang, texts in texts_by_lang.items():
            total_chars = 0
            total_tokens = 0

            for text in texts:
                chars = len(text)
                tokens = len(tokenizer.encode(text))

                total_chars += chars
                total_tokens += tokens

            fertility[lang] = total_tokens / max(total_chars, 1)

        return fertility


def create_multilingual_tokenizer(
    corpus_files: dict,  # {language: file_path}
    vocab_size: int = 100000
):
    """ë‹¤êµ­ì–´ SentencePiece í† í¬ë‚˜ì´ì €"""

    # 1. ì–¸ì–´ë³„ ë°ì´í„° ë³‘í•© (ê°€ì¤‘ì¹˜ ì ìš©)
    merged_file = 'merged_corpus.txt'
    weights = MultilingualTokenizerConfig.LANGUAGE_WEIGHTS

    with open(merged_file, 'w') as out:
        for lang, file_path in corpus_files.items():
            weight = weights.get(lang, 0.01)
            sample_ratio = weight / sum(weights.values())

            with open(file_path, 'r') as f:
                lines = f.readlines()
                n_samples = int(len(lines) * sample_ratio * 10)  # ì˜¤ë²„ìƒ˜í”Œë§

                for line in lines[:n_samples]:
                    out.write(line)

    # 2. SentencePiece í•™ìŠµ
    spm.SentencePieceTrainer.train(
        input=merged_file,
        model_prefix='multilingual',
        vocab_size=vocab_size,
        model_type='bpe',
        character_coverage=0.9995,  # ë†’ì€ ì»¤ë²„ë¦¬ì§€
        byte_fallback=True,
        split_digits=True,
        # íŠ¹ìˆ˜ í† í°
        user_defined_symbols=['<code>', '</code>', '<math>', '</math>'],
    )

    return 'multilingual.model'
```

---

## 7. Tokenizer-Free ëª¨ë¸

### 7.1 ByT5 (Byte-level T5)

```python
class ByteLevelModel:
    """
    ByT5 ìŠ¤íƒ€ì¼ Byte-Level ëª¨ë¸

    íŠ¹ì§•:
    - í† í¬ë‚˜ì´ì € ì—†ìŒ
    - ì…ë ¥: raw UTF-8 bytes (0-255)
    - ì¥ì : ì–¸ì–´ ë…ë¦½ì , ë…¸ì´ì¦ˆì— ê°•í•¨
    - ë‹¨ì : ê¸´ ì‹œí€€ìŠ¤ (3-4ë°°)
    """

    VOCAB_SIZE = 259  # 256 bytes + 3 special tokens

    def __init__(self):
        self.pad_id = 256
        self.eos_id = 257
        self.unk_id = 258

    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ â†’ bytes"""
        return list(text.encode('utf-8'))

    def decode(self, ids: List[int]) -> str:
        """bytes â†’ í…ìŠ¤íŠ¸"""
        # íŠ¹ìˆ˜ í† í° ì œê±°
        bytes_list = [b for b in ids if b < 256]
        return bytes(bytes_list).decode('utf-8', errors='replace')


# ByT5 ì‚¬ìš© ì˜ˆì‹œ
from transformers import AutoTokenizer, T5ForConditionalGeneration

def use_byt5():
    """ByT5 ì‚¬ìš©"""
    tokenizer = AutoTokenizer.from_pretrained("google/byt5-small")
    model = T5ForConditionalGeneration.from_pretrained("google/byt5-small")

    # Byte-level ì¸ì½”ë”©
    text = "translate English to German: Hello, how are you?"
    inputs = tokenizer(text, return_tensors="pt")

    print(f"Text length: {len(text)} chars")
    print(f"Token length: {inputs['input_ids'].shape[1]} tokens")
    # Byte-levelì´ë¯€ë¡œ ëŒ€ëµ ë¹„ìŠ·

    # ìƒì„±
    outputs = model.generate(**inputs, max_length=100)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Result: {result}")
```

### 7.2 MEGABYTE

```
MEGABYTE ì•„í‚¤í…ì²˜:
- Patch-based byte modeling
- Global model: í° transformer, patch ë ˆë²¨
- Local model: ì‘ì€ transformer, byte ë ˆë²¨

ì¥ì :
- ê¸´ byte ì‹œí€€ìŠ¤ íš¨ìœ¨ì  ì²˜ë¦¬
- O(nÂ²) â†’ O(nÂ²/p + p * n) ë³µì¡ë„ (p = patch í¬ê¸°)
```

---

## 8. ì½”ë“œìš© Tokenization

### 8.1 ì½”ë“œ íŠ¹í™” ì „ëµ

```python
class CodeTokenizer:
    """
    í”„ë¡œê·¸ë˜ë° ì½”ë“œìš© í† í¬ë‚˜ì´ì €

    ê³ ë ¤ì‚¬í•­:
    1. ë“¤ì—¬ì“°ê¸° ë³´ì¡´
    2. ì‹ë³„ì ë¶„í•  (camelCase, snake_case)
    3. ìˆ«ì ë¦¬í„°ëŸ´
    4. íŠ¹ìˆ˜ ë¬¸ì (==, !=, <=, etc.)
    """

    def preprocess_code(self, code: str) -> str:
        """ì½”ë“œ ì „ì²˜ë¦¬"""
        # ë“¤ì—¬ì“°ê¸°ë¥¼ íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ
        lines = code.split('\n')
        processed = []

        for line in lines:
            # ë“¤ì—¬ì“°ê¸° ê³„ì‚°
            indent = len(line) - len(line.lstrip())
            indent_tokens = '<INDENT>' * (indent // 4)

            processed.append(indent_tokens + line.lstrip())

        return '\n'.join(processed)

    def split_identifier(self, identifier: str) -> List[str]:
        """ì‹ë³„ì ë¶„í• """
        # camelCase
        import re
        tokens = re.sub('([A-Z])', r' \1', identifier).split()

        # snake_case
        result = []
        for token in tokens:
            result.extend(token.split('_'))

        return [t for t in result if t]


# Codex/StarCoder ìŠ¤íƒ€ì¼
def create_code_tokenizer():
    """ì½”ë“œìš© í† í¬ë‚˜ì´ì € (StarCoder ìŠ¤íƒ€ì¼)"""
    from tokenizers import Tokenizer, models, trainers, pre_tokenizers

    tokenizer = Tokenizer(models.BPE())

    # ì½”ë“œìš© pre-tokenization
    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
        pre_tokenizers.ByteLevel(add_prefix_space=False),
        # ìˆ«ì ë¶„ë¦¬
        pre_tokenizers.Digits(individual_digits=True),
    ])

    # íŠ¹ìˆ˜ í† í°
    special_tokens = [
        '<|endoftext|>',
        '<fim_prefix>',  # Fill-in-the-middle
        '<fim_middle>',
        '<fim_suffix>',
        '<filename>',
        '<gh_stars>',
        '<issue_start>',
        '<issue_comment>',
        '<issue_closed>',
        '<jupyter_start>',
        '<jupyter_code>',
        '<jupyter_output>',
        '<empty_output>',
        '<commit_before>',
        '<commit_msg>',
        '<commit_after>',
    ]

    trainer = trainers.BpeTrainer(
        vocab_size=49152,
        special_tokens=special_tokens,
    )

    return tokenizer, trainer
```

---

## 9. ì‹¤ìŠµ: í† í¬ë‚˜ì´ì € ë¶„ì„

```python
from transformers import AutoTokenizer
import matplotlib.pyplot as plt

def analyze_tokenizers():
    """ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ë¹„êµ ë¶„ì„"""

    tokenizers = {
        'GPT-2': AutoTokenizer.from_pretrained('gpt2'),
        'BERT': AutoTokenizer.from_pretrained('bert-base-uncased'),
        'T5': AutoTokenizer.from_pretrained('t5-base'),
        'LLaMA': AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf'),
    }

    test_texts = {
        'English': "The quick brown fox jumps over the lazy dog.",
        'Korean': "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
        'Code': "def hello_world():\n    print('Hello, World!')",
        'Math': "The equation e^(iÏ€) + 1 = 0 is beautiful.",
        'Mixed': "I love eating ê¹€ì¹˜ with rice ğŸš",
    }

    # ë¶„ì„
    results = {}
    for tok_name, tokenizer in tokenizers.items():
        results[tok_name] = {}

        for text_name, text in test_texts.items():
            try:
                tokens = tokenizer.tokenize(text)
                ids = tokenizer.encode(text)

                results[tok_name][text_name] = {
                    'n_tokens': len(tokens),
                    'n_chars': len(text),
                    'fertility': len(tokens) / len(text),
                    'tokens': tokens[:10],  # ì²˜ìŒ 10ê°œë§Œ
                }
            except:
                results[tok_name][text_name] = None

    # ì¶œë ¥
    for tok_name, tok_results in results.items():
        print(f"\n{'='*50}")
        print(f"Tokenizer: {tok_name}")
        print('='*50)

        for text_name, result in tok_results.items():
            if result:
                print(f"\n{text_name}:")
                print(f"  Tokens: {result['n_tokens']}")
                print(f"  Chars: {result['n_chars']}")
                print(f"  Fertility: {result['fertility']:.3f}")
                print(f"  Sample: {result['tokens']}")

    # Fertility ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 6))

    x = list(test_texts.keys())
    width = 0.2
    positions = range(len(x))

    for i, (tok_name, tok_results) in enumerate(results.items()):
        fertilities = [
            tok_results[text_name]['fertility'] if tok_results.get(text_name) else 0
            for text_name in x
        ]
        offset = (i - len(results) / 2) * width
        ax.bar([p + offset for p in positions], fertilities, width, label=tok_name)

    ax.set_xlabel('Text Type')
    ax.set_ylabel('Fertility (tokens/chars)')
    ax.set_title('Tokenizer Fertility Comparison')
    ax.set_xticks(positions)
    ax.set_xticklabels(x)
    ax.legend()

    plt.tight_layout()
    plt.savefig('tokenizer_comparison.png')
    plt.show()


if __name__ == "__main__":
    analyze_tokenizers()
```

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- Sennrich et al. (2016). "Neural Machine Translation of Rare Words with Subword Units" (BPE)
- Kudo & Richardson (2018). "SentencePiece: A simple and language independent subword tokenizer"
- Xue et al. (2021). "ByT5: Towards a token-free future with pre-trained byte-to-byte models"

### ë„êµ¬
- [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers)
- [SentencePiece](https://github.com/google/sentencepiece)
- [tiktoken](https://github.com/openai/tiktoken) (OpenAI)

### ê´€ë ¨ ë ˆìŠ¨
- [../LLM_and_NLP/01_NLP_Basics.md](../LLM_and_NLP/01_NLP_Basics.md)
