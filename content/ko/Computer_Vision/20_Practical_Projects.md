# ì‹¤ì „ í”„ë¡œì íŠ¸ (Practical Projects)

## ê°œìš”

ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ OpenCV ê¸°ìˆ ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì œ ì‘ìš© í”„ë¡œì íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ê° í”„ë¡œì íŠ¸ëŠ” ì—¬ëŸ¬ ê¸°ìˆ ì„ ì¡°í•©í•˜ì—¬ ì™„ì„±ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.

**ë‚œì´ë„**: â­â­â­â­

**ì„ ìˆ˜ ì§€ì‹**: ì´ì „ ëª¨ë“  ì±•í„°ì˜ ë‚´ìš©

---

## ëª©ì°¨

1. [í”„ë¡œì íŠ¸ 1: ë¬¸ì„œ ìŠ¤ìºë„ˆ](#í”„ë¡œì íŠ¸-1-ë¬¸ì„œ-ìŠ¤ìºë„ˆ)
2. [í”„ë¡œì íŠ¸ 2: ì°¨ì„  ê²€ì¶œ](#í”„ë¡œì íŠ¸-2-ì°¨ì„ -ê²€ì¶œ)
3. [í”„ë¡œì íŠ¸ 3: AR ë§ˆì»¤ ê²€ì¶œ](#í”„ë¡œì íŠ¸-3-ar-ë§ˆì»¤-ê²€ì¶œ)
4. [í”„ë¡œì íŠ¸ 4: ì‹¤ì‹œê°„ ì–¼êµ´ í•„í„°](#í”„ë¡œì íŠ¸-4-ì‹¤ì‹œê°„-ì–¼êµ´-í•„í„°)
5. [í”„ë¡œì íŠ¸ 5: ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ](#í”„ë¡œì íŠ¸-5-ê°ì²´-ì¶”ì -ì‹œìŠ¤í…œ)
6. [ì—°ìŠµ ë¬¸ì œ ë° í™•ì¥ ì•„ì´ë””ì–´](#ì—°ìŠµ-ë¬¸ì œ-ë°-í™•ì¥-ì•„ì´ë””ì–´)

---

## í”„ë¡œì íŠ¸ 1: ë¬¸ì„œ ìŠ¤ìºë„ˆ

### í”„ë¡œì íŠ¸ ê°œìš”

```
ë¬¸ì„œ ìŠ¤ìºë„ˆ (Document Scanner):
ì‚¬ì§„ìœ¼ë¡œ ì°ì€ ë¬¸ì„œë¥¼ ì •ë ¬ëœ ìŠ¤ìº” ì´ë¯¸ì§€ë¡œ ë³€í™˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ì´¬ì˜ëœ ë¬¸ì„œ     â”‚        â”‚   ìŠ¤ìº”ëœ ê²°ê³¼    â”‚
â”‚  /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\   â”‚        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ /             \  â”‚  â”€â”€â–¶   â”‚ â”‚              â”‚ â”‚
â”‚ \             /  â”‚        â”‚ â”‚   ë¬¸ì„œ ë‚´ìš©   â”‚ â”‚
â”‚  \___________/   â”‚        â”‚ â”‚              â”‚ â”‚
â”‚                  â”‚        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     ê¸°ìš¸ì–´ì§„ ì›ë³¸                ì •ë ¬ëœ ê²°ê³¼

ì‚¬ìš© ê¸°ìˆ :
- ì—£ì§€ ê²€ì¶œ (Canny)
- ìœ¤ê³½ì„  ê²€ì¶œ (findContours)
- ë‹¤ê°í˜• ê·¼ì‚¬ (approxPolyDP)
- ì›ê·¼ ë³€í™˜ (warpPerspective)
- ì´ì§„í™” (adaptiveThreshold)
```

### ë‹¨ê³„ë³„ êµ¬í˜„

```python
import cv2
import numpy as np

class DocumentScanner:
    """ë¬¸ì„œ ìŠ¤ìºë„ˆ"""

    def __init__(self):
        pass

    def order_points(self, pts):
        """4ê°œì˜ ì ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì¢Œìƒ, ìš°ìƒ, ìš°í•˜, ì¢Œí•˜)"""
        rect = np.zeros((4, 2), dtype=np.float32)

        # ì¢Œìƒ: x+y í•©ì´ ê°€ì¥ ì‘ìŒ
        # ìš°í•˜: x+y í•©ì´ ê°€ì¥ í¼
        s = pts.sum(axis=1)
        rect[0] = pts[np.argmin(s)]
        rect[2] = pts[np.argmax(s)]

        # ìš°ìƒ: y-x ì°¨ì´ê°€ ê°€ì¥ ì‘ìŒ
        # ì¢Œí•˜: y-x ì°¨ì´ê°€ ê°€ì¥ í¼
        diff = np.diff(pts, axis=1)
        rect[1] = pts[np.argmin(diff)]
        rect[3] = pts[np.argmax(diff)]

        return rect

    def four_point_transform(self, image, pts):
        """ì›ê·¼ ë³€í™˜ìœ¼ë¡œ ë¬¸ì„œ ì •ë ¬"""
        rect = self.order_points(pts)
        (tl, tr, br, bl) = rect

        # ìƒˆ ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚°
        width_a = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
        width_b = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
        max_width = max(int(width_a), int(width_b))

        height_a = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
        height_b = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
        max_height = max(int(height_a), int(height_b))

        # ëª©í‘œ ì¢Œí‘œ
        dst = np.array([
            [0, 0],
            [max_width - 1, 0],
            [max_width - 1, max_height - 1],
            [0, max_height - 1]
        ], dtype=np.float32)

        # ì›ê·¼ ë³€í™˜ í–‰ë ¬
        M = cv2.getPerspectiveTransform(rect, dst)
        warped = cv2.warpPerspective(image, M, (max_width, max_height))

        return warped

    def find_document_contour(self, image):
        """ë¬¸ì„œ ìœ¤ê³½ì„  ì°¾ê¸°"""
        # ì „ì²˜ë¦¬
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)

        # ì—£ì§€ ê²€ì¶œ
        edged = cv2.Canny(blur, 75, 200)

        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ì—£ì§€ ì—°ê²°
        kernel = np.ones((5, 5), np.uint8)
        edged = cv2.dilate(edged, kernel, iterations=1)
        edged = cv2.erode(edged, kernel, iterations=1)

        # ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL,
                                        cv2.CHAIN_APPROX_SIMPLE)

        # ê°€ì¥ í° 4ê°í˜• ìœ¤ê³½ì„  ì°¾ê¸°
        contours = sorted(contours, key=cv2.contourArea, reverse=True)

        document_contour = None
        for contour in contours[:5]:  # ìƒìœ„ 5ê°œë§Œ í™•ì¸
            peri = cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, 0.02 * peri, True)

            if len(approx) == 4:
                document_contour = approx
                break

        return document_contour, edged

    def enhance_document(self, image):
        """ë¬¸ì„œ ì´ë¯¸ì§€ í–¥ìƒ"""
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # ì ì‘í˜• ì´ì§„í™”
        binary = cv2.adaptiveThreshold(
            gray, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            11, 2
        )

        # ë˜ëŠ” OTSU ì´ì§„í™”
        # _, binary = cv2.threshold(gray, 0, 255,
        #                           cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        return binary

    def scan(self, image, enhance=True):
        """ë¬¸ì„œ ìŠ¤ìº” ì „ì²´ ê³¼ì •"""
        original = image.copy()
        height, width = image.shape[:2]

        # ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
        ratio = 500.0 / height
        resized = cv2.resize(image, None, fx=ratio, fy=ratio)

        # ë¬¸ì„œ ìœ¤ê³½ì„  ì°¾ê¸°
        contour, edged = self.find_document_contour(resized)

        if contour is None:
            print("ë¬¸ì„œ ìœ¤ê³½ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None, None

        # ì›ë³¸ í¬ê¸°ë¡œ ì¢Œí‘œ ë³€í™˜
        contour = contour.reshape(4, 2) / ratio

        # ì›ê·¼ ë³€í™˜
        scanned = self.four_point_transform(original, contour)

        # ë¬¸ì„œ í–¥ìƒ (ì„ íƒì‚¬í•­)
        if enhance:
            scanned = self.enhance_document(scanned)

        return scanned, contour

    def visualize(self, image, contour):
        """ê²°ê³¼ ì‹œê°í™”"""
        vis = image.copy()
        if contour is not None:
            cv2.drawContours(vis, [contour.astype(int)], -1, (0, 255, 0), 3)

            # ì½”ë„ˆ ì  í‘œì‹œ
            for point in contour:
                cv2.circle(vis, tuple(point.astype(int)), 10, (0, 0, 255), -1)

        return vis

# ì‚¬ìš© ì˜ˆ
scanner = DocumentScanner()

# ì´ë¯¸ì§€ ë¡œë“œ
img = cv2.imread('document_photo.jpg')

# ìŠ¤ìº”
scanned, contour = scanner.scan(img, enhance=True)

if scanned is not None:
    # ê²°ê³¼ ì‹œê°í™”
    vis = scanner.visualize(img, contour)

    cv2.imshow('Original with Contour', vis)
    cv2.imshow('Scanned', scanned)
    cv2.waitKey(0)

    # ì €ì¥
    cv2.imwrite('scanned_document.jpg', scanned)
```

### ì‹¤ì‹œê°„ ë¬¸ì„œ ìŠ¤ìºë„ˆ

```python
import cv2
import numpy as np

def realtime_document_scanner():
    """ì‹¤ì‹œê°„ ë¬¸ì„œ ìŠ¤ìºë„ˆ"""

    scanner = DocumentScanner()
    cap = cv2.VideoCapture(0)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # ë¬¸ì„œ ìœ¤ê³½ì„  ê²€ì¶œ
        height = frame.shape[0]
        ratio = 500.0 / height
        resized = cv2.resize(frame, None, fx=ratio, fy=ratio)

        contour, _ = scanner.find_document_contour(resized)

        display = frame.copy()

        if contour is not None:
            # ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜
            contour = (contour.reshape(4, 2) / ratio).astype(int)

            # ìœ¤ê³½ì„  ê·¸ë¦¬ê¸°
            cv2.drawContours(display, [contour], -1, (0, 255, 0), 3)

            # ì•ˆë‚´ í…ìŠ¤íŠ¸
            cv2.putText(display, "Press 's' to scan", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        else:
            cv2.putText(display, "Document not detected", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        cv2.imshow('Document Scanner', display)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s') and contour is not None:
            # ìŠ¤ìº” ìˆ˜í–‰
            scanned, _ = scanner.scan(frame)
            if scanned is not None:
                cv2.imshow('Scanned', scanned)
                cv2.imwrite('scanned.jpg', scanned)

    cap.release()
    cv2.destroyAllWindows()

# ì‹¤í–‰
# realtime_document_scanner()
```

---

## í”„ë¡œì íŠ¸ 2: ì°¨ì„  ê²€ì¶œ

### í”„ë¡œì íŠ¸ ê°œìš”

```
ì°¨ì„  ê²€ì¶œ (Lane Detection):
ë„ë¡œ ì˜ìƒì—ì„œ ì°¨ì„ ì„ ê²€ì¶œí•˜ê³  ì‹œê°í™”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ë„ë¡œ ì˜ìƒ               â”‚
â”‚                                    â”‚
â”‚     â•²                    â•±         â”‚
â”‚      â•²      ì°¨ì„        â•±          â”‚
â”‚       â•²              â•±            â”‚
â”‚        â•²    ê²€ì¶œ   â•±              â”‚
â”‚         â•²        â•±                â”‚
â”‚          â•²      â•±                 â”‚
â”‚           â•²    â•±                  â”‚
â”‚            â•²  â•±                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
1. ê´€ì‹¬ ì˜ì—­ (ROI) ì„¤ì •
2. ìƒ‰ìƒ ê³µê°„ ë³€í™˜ (HSV)
3. í°ìƒ‰/ë…¸ë€ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
4. ìºë‹ˆ ì—£ì§€ ê²€ì¶œ
5. í—ˆí”„ ë³€í™˜ìœ¼ë¡œ ì§ì„  ê²€ì¶œ
6. ì°¨ì„  í•©ì„±
```

### ë‹¨ê³„ë³„ êµ¬í˜„

```python
import cv2
import numpy as np

class LaneDetector:
    """ì°¨ì„  ê²€ì¶œê¸°"""

    def __init__(self):
        pass

    def region_of_interest(self, img):
        """ê´€ì‹¬ ì˜ì—­ ë§ˆìŠ¤í‚¹ (ë„ë¡œ ë¶€ë¶„ë§Œ)"""
        height, width = img.shape[:2]

        # ì‚¬ë‹¤ë¦¬ê¼´ ROI
        vertices = np.array([[
            (int(width * 0.1), height),           # ì¢Œí•˜
            (int(width * 0.4), int(height * 0.6)), # ì¢Œìƒ
            (int(width * 0.6), int(height * 0.6)), # ìš°ìƒ
            (int(width * 0.9), height)            # ìš°í•˜
        ]], dtype=np.int32)

        mask = np.zeros_like(img)

        if len(img.shape) == 3:
            cv2.fillPoly(mask, vertices, (255, 255, 255))
        else:
            cv2.fillPoly(mask, vertices, 255)

        masked = cv2.bitwise_and(img, mask)
        return masked

    def color_filter(self, img):
        """ìƒ‰ìƒ í•„í„° (í°ìƒ‰/ë…¸ë€ìƒ‰ ì°¨ì„ )"""
        # HSV ë³€í™˜
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

        # í°ìƒ‰ ë§ˆìŠ¤í¬
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([255, 30, 255])
        white_mask = cv2.inRange(hsv, lower_white, upper_white)

        # ë…¸ë€ìƒ‰ ë§ˆìŠ¤í¬
        lower_yellow = np.array([15, 80, 100])
        upper_yellow = np.array([35, 255, 255])
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)

        # ë§ˆìŠ¤í¬ ê²°í•©
        combined_mask = cv2.bitwise_or(white_mask, yellow_mask)

        # ë§ˆìŠ¤í¬ ì ìš©
        filtered = cv2.bitwise_and(img, img, mask=combined_mask)

        return filtered, combined_mask

    def detect_edges(self, img):
        """ì—£ì§€ ê²€ì¶œ"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)
        return edges

    def detect_lines(self, edges):
        """í—ˆí”„ ë³€í™˜ìœ¼ë¡œ ì§ì„  ê²€ì¶œ"""
        lines = cv2.HoughLinesP(
            edges,
            rho=1,              # ê±°ë¦¬ í•´ìƒë„ (í”½ì…€)
            theta=np.pi/180,    # ê°ë„ í•´ìƒë„ (ë¼ë””ì•ˆ)
            threshold=50,       # ìµœì†Œ íˆ¬í‘œ ìˆ˜
            minLineLength=50,   # ìµœì†Œ ì„  ê¸¸ì´
            maxLineGap=150      # ìµœëŒ€ ê°„ê²©
        )
        return lines

    def separate_lines(self, lines, img_width):
        """ì¢Œ/ìš° ì°¨ì„  ë¶„ë¦¬"""
        left_lines = []
        right_lines = []

        if lines is None:
            return left_lines, right_lines

        center = img_width / 2

        for line in lines:
            x1, y1, x2, y2 = line[0]

            # ê¸°ìš¸ê¸° ê³„ì‚°
            if x2 - x1 == 0:
                continue

            slope = (y2 - y1) / (x2 - x1)

            # ê¸°ìš¸ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬´ì‹œ (ìˆ˜í‰ì„ )
            if abs(slope) < 0.3:
                continue

            # ì¢Œ/ìš° ë¶„ë¥˜
            if slope < 0 and x1 < center and x2 < center:
                left_lines.append(line[0])
            elif slope > 0 and x1 > center and x2 > center:
                right_lines.append(line[0])

        return left_lines, right_lines

    def average_line(self, lines, img_height):
        """ì—¬ëŸ¬ ì„ ë¶„ì„ í‰ê· ë‚´ì–´ í•˜ë‚˜ì˜ ì„ ìœ¼ë¡œ"""
        if len(lines) == 0:
            return None

        x_coords = []
        y_coords = []

        for line in lines:
            x1, y1, x2, y2 = line
            x_coords.extend([x1, x2])
            y_coords.extend([y1, y2])

        # ì„ í˜• íšŒê·€ (1ì°¨ ë‹¤í•­ì‹ í”¼íŒ…)
        poly = np.polyfit(y_coords, x_coords, deg=1)

        # y ë²”ìœ„ ì„¤ì •
        y1 = img_height
        y2 = int(img_height * 0.6)

        # x ì¢Œí‘œ ê³„ì‚°
        x1 = int(np.polyval(poly, y1))
        x2 = int(np.polyval(poly, y2))

        return [x1, y1, x2, y2]

    def draw_lanes(self, img, left_line, right_line):
        """ì°¨ì„  ê·¸ë¦¬ê¸°"""
        overlay = np.zeros_like(img)

        # ì°¨ì„  ê·¸ë¦¬ê¸°
        if left_line is not None:
            cv2.line(overlay, (left_line[0], left_line[1]),
                    (left_line[2], left_line[3]), (0, 0, 255), 10)

        if right_line is not None:
            cv2.line(overlay, (right_line[0], right_line[1]),
                    (right_line[2], right_line[3]), (0, 0, 255), 10)

        # ì°¨ì„  ì˜ì—­ ì±„ìš°ê¸°
        if left_line is not None and right_line is not None:
            pts = np.array([
                [left_line[0], left_line[1]],
                [left_line[2], left_line[3]],
                [right_line[2], right_line[3]],
                [right_line[0], right_line[1]]
            ], np.int32)

            cv2.fillPoly(overlay, [pts], (0, 255, 0))

        # ì›ë³¸ê³¼ í•©ì„±
        result = cv2.addWeighted(img, 1, overlay, 0.3, 0)

        return result

    def detect(self, img):
        """ì „ì²´ ì°¨ì„  ê²€ì¶œ íŒŒì´í”„ë¼ì¸"""
        height, width = img.shape[:2]

        # 1. ìƒ‰ìƒ í•„í„°ë§
        filtered, color_mask = self.color_filter(img)

        # 2. ì—£ì§€ ê²€ì¶œ
        edges = self.detect_edges(filtered)

        # 3. ROI ì ìš©
        roi_edges = self.region_of_interest(edges)

        # 4. ì§ì„  ê²€ì¶œ
        lines = self.detect_lines(roi_edges)

        # 5. ì¢Œ/ìš° ì°¨ì„  ë¶„ë¦¬
        left_lines, right_lines = self.separate_lines(lines, width)

        # 6. í‰ê·  ì°¨ì„  ê³„ì‚°
        left_lane = self.average_line(left_lines, height)
        right_lane = self.average_line(right_lines, height)

        # 7. ê²°ê³¼ ì‹œê°í™”
        result = self.draw_lanes(img, left_lane, right_lane)

        return result, {
            'edges': roi_edges,
            'color_mask': color_mask,
            'left_lane': left_lane,
            'right_lane': right_lane
        }

# ì‚¬ìš© ì˜ˆ
detector = LaneDetector()

# ì´ë¯¸ì§€ì—ì„œ ì°¨ì„  ê²€ì¶œ
img = cv2.imread('road.jpg')
result, debug = detector.detect(img)

cv2.imshow('Lane Detection', result)
cv2.imshow('Edges', debug['edges'])
cv2.waitKey(0)
```

### ë¹„ë””ì˜¤ ì°¨ì„  ê²€ì¶œ

```python
import cv2
import numpy as np

def video_lane_detection(video_path):
    """ë¹„ë””ì˜¤ì—ì„œ ì°¨ì„  ê²€ì¶œ"""

    detector = LaneDetector()
    cap = cv2.VideoCapture(video_path)

    # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('lane_output.mp4', fourcc, fps, (width, height))

    # ì´ì „ í”„ë ˆì„ì˜ ì°¨ì„  (ìŠ¤ë¬´ë”©ìš©)
    prev_left = None
    prev_right = None
    alpha = 0.7  # ìŠ¤ë¬´ë”© ê³„ìˆ˜

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        result, debug = detector.detect(frame)

        # ì°¨ì„  ìŠ¤ë¬´ë”© (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
        left = debug['left_lane']
        right = debug['right_lane']

        if prev_left is not None and left is not None:
            left = [int(alpha * prev_left[i] + (1 - alpha) * left[i])
                    for i in range(4)]
        if prev_right is not None and right is not None:
            right = [int(alpha * prev_right[i] + (1 - alpha) * right[i])
                     for i in range(4)]

        prev_left = left
        prev_right = right

        # ìŠ¤ë¬´ë”©ëœ ì°¨ì„ ìœ¼ë¡œ ë‹¤ì‹œ ê·¸ë¦¬ê¸°
        result = detector.draw_lanes(frame, left, right)

        out.write(result)
        cv2.imshow('Lane Detection', result)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

# ì‹¤í–‰
# video_lane_detection('driving.mp4')
```

---

## í”„ë¡œì íŠ¸ 3: AR ë§ˆì»¤ ê²€ì¶œ

### í”„ë¡œì íŠ¸ ê°œìš”

```
AR ë§ˆì»¤ ê²€ì¶œ (AR Marker Detection):
ì´ë¯¸ì§€ì—ì„œ ì •ì‚¬ê°í˜• ë§ˆì»¤ë¥¼ ê²€ì¶œí•˜ê³  3D ê°ì²´ë¥¼ í•©ì„±

ë§ˆì»¤ êµ¬ì¡°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆ                â–ˆ â”‚
â”‚ â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆ â”‚
â”‚ â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆ â”‚
â”‚ â–ˆ                â–ˆ â”‚
â”‚ â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆ â”‚
â”‚ â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆ â”‚
â”‚ â–ˆ                â–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì²˜ë¦¬ ê³¼ì •:
1. ì‚¬ê°í˜• ìœ¤ê³½ì„  ê²€ì¶œ
2. ì›ê·¼ ë³€í™˜ìœ¼ë¡œ ë§ˆì»¤ ì •ê·œí™”
3. ë§ˆì»¤ ID ì¸ì‹
4. í˜¸ëª¨ê·¸ë˜í”¼ë¡œ 3D ê°ì²´ íˆ¬ì˜
```

### ë‹¨ê³„ë³„ êµ¬í˜„

```python
import cv2
import numpy as np

class ARMarkerDetector:
    """AR ë§ˆì»¤ ê²€ì¶œê¸°"""

    def __init__(self, marker_size=100):
        self.marker_size = marker_size

    def order_points(self, pts):
        """4ê°œ ì ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬"""
        rect = np.zeros((4, 2), dtype=np.float32)

        s = pts.sum(axis=1)
        rect[0] = pts[np.argmin(s)]  # ì¢Œìƒ
        rect[2] = pts[np.argmax(s)]  # ìš°í•˜

        diff = np.diff(pts, axis=1)
        rect[1] = pts[np.argmin(diff)]  # ìš°ìƒ
        rect[3] = pts[np.argmax(diff)]  # ì¢Œí•˜

        return rect

    def find_markers(self, img):
        """ë§ˆì»¤ í›„ë³´ ì°¾ê¸°"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)

        # ì ì‘í˜• ì´ì§„í™”
        binary = cv2.adaptiveThreshold(
            blur, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV,
            11, 2
        )

        # ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(binary, cv2.RETR_LIST,
                                        cv2.CHAIN_APPROX_SIMPLE)

        markers = []

        for contour in contours:
            # ë©´ì  í•„í„°
            area = cv2.contourArea(contour)
            if area < 1000 or area > img.shape[0] * img.shape[1] * 0.5:
                continue

            # ë‹¤ê°í˜• ê·¼ì‚¬
            peri = cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, 0.04 * peri, True)

            # 4ê°í˜•ì¸ ê²½ìš°ë§Œ
            if len(approx) == 4:
                # ë³¼ë¡ ë‹¤ê°í˜• í™•ì¸
                if cv2.isContourConvex(approx):
                    markers.append(approx.reshape(4, 2))

        return markers, binary

    def get_marker_transform(self, corners):
        """ë§ˆì»¤ ì •ê·œí™”ë¥¼ ìœ„í•œ ë³€í™˜ í–‰ë ¬"""
        ordered = self.order_points(corners.astype(np.float32))

        dst = np.array([
            [0, 0],
            [self.marker_size - 1, 0],
            [self.marker_size - 1, self.marker_size - 1],
            [0, self.marker_size - 1]
        ], dtype=np.float32)

        M = cv2.getPerspectiveTransform(ordered, dst)
        return M, ordered

    def decode_marker(self, warped):
        """ë§ˆì»¤ ID ë””ì½”ë”© (ê°„ë‹¨í•œ ì˜ˆ)"""
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if len(warped.shape) == 3:
            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)

        # ì´ì§„í™”
        _, binary = cv2.threshold(warped, 127, 255, cv2.THRESH_BINARY)

        # 5x5 ê·¸ë¦¬ë“œë¡œ ë¶„í•  (ê°€ì¥ìë¦¬ëŠ” ê²€ì€ í…Œë‘ë¦¬)
        grid_size = self.marker_size // 5
        grid = np.zeros((5, 5), dtype=np.uint8)

        for i in range(5):
            for j in range(5):
                cell = binary[i*grid_size:(i+1)*grid_size,
                             j*grid_size:(j+1)*grid_size]
                # ì…€ì˜ í‰ê·  ë°ê¸°ë¡œ 0/1 ê²°ì •
                grid[i, j] = 1 if np.mean(cell) > 127 else 0

        # ê°„ë‹¨í•œ ID ê³„ì‚° (ë‚´ë¶€ 3x3 ì˜ì—­)
        inner = grid[1:4, 1:4]
        marker_id = 0
        for i in range(3):
            for j in range(3):
                marker_id = marker_id * 2 + inner[i, j]

        return marker_id, grid

    def draw_cube(self, img, corners, size=50):
        """ë§ˆì»¤ ìœ„ì— 3D íë¸Œ ê·¸ë¦¬ê¸°"""
        # ë§ˆì»¤ í‰ë©´ì˜ 4ê°œ ì 
        corners = self.order_points(corners.astype(np.float32))

        # ë°”ë‹¥ë©´ ì¢Œí‘œ
        bottom = corners.astype(int)

        # ìœ—ë©´ ì¢Œí‘œ ê³„ì‚° (í˜¸ëª¨ê·¸ë˜í”¼ ì´ìš©í•œ ê°„ë‹¨í•œ ê·¼ì‚¬)
        center = np.mean(corners, axis=0)

        # ìœ—ë©´ì€ ë§ˆì»¤ ì¤‘ì‹¬ ë°©í–¥ìœ¼ë¡œ ì¶•ì†Œ + ìœ„ë¡œ ì´ë™
        scale = 0.7
        offset = np.array([0, -size])  # ìœ„ë¡œ ì´ë™

        top = []
        for pt in corners:
            vec = pt - center
            new_pt = center + vec * scale + offset
            top.append(new_pt.astype(int))
        top = np.array(top)

        # ë©´ ê·¸ë¦¬ê¸° (ë°˜íˆ¬ëª…)
        overlay = img.copy()

        # ìœ—ë©´ (ë¹¨ê°„ìƒ‰)
        cv2.fillPoly(overlay, [top], (0, 0, 200))

        # ì˜†ë©´ (ë…¹ìƒ‰)
        for i in range(4):
            pts = np.array([bottom[i], bottom[(i+1)%4],
                           top[(i+1)%4], top[i]])
            cv2.fillPoly(overlay, [pts], (0, 200, 0))

        # í•©ì„±
        result = cv2.addWeighted(img, 0.6, overlay, 0.4, 0)

        # ì—£ì§€ ê·¸ë¦¬ê¸°
        for i in range(4):
            cv2.line(result, tuple(bottom[i]), tuple(bottom[(i+1)%4]),
                    (255, 255, 255), 2)
            cv2.line(result, tuple(top[i]), tuple(top[(i+1)%4]),
                    (255, 255, 255), 2)
            cv2.line(result, tuple(bottom[i]), tuple(top[i]),
                    (255, 255, 255), 2)

        return result

    def detect(self, img):
        """ë§ˆì»¤ ê²€ì¶œ ë° AR ë Œë”ë§"""
        result = img.copy()

        markers, binary = self.find_markers(img)

        detected_markers = []

        for corners in markers:
            # ë§ˆì»¤ ì •ê·œí™”
            M, ordered = self.get_marker_transform(corners)
            warped = cv2.warpPerspective(img, M,
                                         (self.marker_size, self.marker_size))

            # ë§ˆì»¤ ID ë””ì½”ë”©
            marker_id, grid = self.decode_marker(warped)

            # í…Œë‘ë¦¬ í™•ì¸ (ê°€ì¥ìë¦¬ê°€ ê²€ì€ìƒ‰ì´ì–´ì•¼ í•¨)
            border_check = (grid[0, :].sum() + grid[4, :].sum() +
                           grid[:, 0].sum() + grid[:, 4].sum())

            if border_check < 5:  # ëŒ€ë¶€ë¶„ ê²€ì€ìƒ‰
                detected_markers.append({
                    'id': marker_id,
                    'corners': ordered
                })

                # 3D íë¸Œ ê·¸ë¦¬ê¸°
                result = self.draw_cube(result, ordered)

                # ID í‘œì‹œ
                center = np.mean(ordered, axis=0).astype(int)
                cv2.putText(result, f"ID: {marker_id}", tuple(center),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)

        return result, detected_markers, binary

# ì‚¬ìš© ì˜ˆ
detector = ARMarkerDetector()

# ì´ë¯¸ì§€ì—ì„œ ë§ˆì»¤ ê²€ì¶œ
img = cv2.imread('ar_marker.jpg')
result, markers, binary = detector.detect(img)

print(f"ê²€ì¶œëœ ë§ˆì»¤: {len(markers)}")
for m in markers:
    print(f"  ID: {m['id']}")

cv2.imshow('AR Detection', result)
cv2.imshow('Binary', binary)
cv2.waitKey(0)
```

### ArUco ë§ˆì»¤ ì‚¬ìš© (OpenCV ë‚´ì¥)

```python
import cv2
import numpy as np

def aruco_marker_detection():
    """OpenCV ArUco ë§ˆì»¤ ê²€ì¶œ"""

    # ArUco ë”•ì…”ë„ˆë¦¬ ì„ íƒ
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    parameters = cv2.aruco.DetectorParameters()

    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)

    cap = cv2.VideoCapture(0)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # ë§ˆì»¤ ê²€ì¶œ
        corners, ids, rejected = detector.detectMarkers(frame)

        # ê²°ê³¼ ì‹œê°í™”
        if ids is not None:
            cv2.aruco.drawDetectedMarkers(frame, corners, ids)

            for i, corner in enumerate(corners):
                # ê° ë§ˆì»¤ì— íë¸Œ ë˜ëŠ” ì¶• ê·¸ë¦¬ê¸°
                # (ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ìˆëŠ” ê²½ìš°)
                pass

        cv2.imshow('ArUco Detection', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

def generate_aruco_marker(marker_id=0, size=200):
    """ArUco ë§ˆì»¤ ìƒì„±"""
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    marker_img = cv2.aruco.generateImageMarker(aruco_dict, marker_id, size)

    cv2.imwrite(f'aruco_marker_{marker_id}.png', marker_img)
    return marker_img

# ë§ˆì»¤ ìƒì„±
# marker = generate_aruco_marker(0)
# cv2.imshow('Marker', marker)
```

---

## í”„ë¡œì íŠ¸ 4: ì‹¤ì‹œê°„ ì–¼êµ´ í•„í„°

### í”„ë¡œì íŠ¸ ê°œìš”

```
ì‹¤ì‹œê°„ ì–¼êµ´ í•„í„° (Face Filter):
ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•„í„° íš¨ê³¼ ì ìš©

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â”‚
â”‚        ğŸ˜ ì„ ê¸€ë¼ìŠ¤ í•„í„°            â”‚
â”‚       /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\           â”‚
â”‚      â”‚  â—â”€â”€â”€â”€â”€â”€â”€â”€â—   â”‚           â”‚
â”‚      â”‚   \      /    â”‚           â”‚
â”‚       \    â–½       /            â”‚
â”‚        \   âˆª     /              â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì‚¬ìš© ê¸°ìˆ :
- dlib ì–¼êµ´ ëœë“œë§ˆí¬ (68ì )
- íˆ¬ëª… ì´ë¯¸ì§€ í•©ì„±
- ì–´íŒŒì¸/ì›ê·¼ ë³€í™˜
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”
```

### ë‹¨ê³„ë³„ êµ¬í˜„

```python
import cv2
import numpy as np
import dlib

class FaceFilter:
    """ì‹¤ì‹œê°„ ì–¼êµ´ í•„í„°"""

    def __init__(self, predictor_path):
        self.detector = dlib.get_frontal_face_detector()
        self.predictor = dlib.shape_predictor(predictor_path)

        # í•„í„° ì´ë¯¸ì§€ ë¡œë“œ
        self.filters = {}

    def load_filter(self, name, image_path, alpha_path=None):
        """í•„í„° ì´ë¯¸ì§€ ë¡œë“œ (PNG with alpha ê¶Œì¥)"""
        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

        if img.shape[2] == 4:
            # ì´ë¯¸ ì•ŒíŒŒ ì±„ë„ ìˆìŒ
            self.filters[name] = img
        else:
            # ì•ŒíŒŒ ì±„ë„ ì¶”ê°€ (í°ìƒ‰ ë°°ê²½ì„ íˆ¬ëª…ìœ¼ë¡œ)
            if alpha_path:
                alpha = cv2.imread(alpha_path, cv2.IMREAD_GRAYSCALE)
            else:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                _, alpha = cv2.threshold(gray, 250, 255,
                                         cv2.THRESH_BINARY_INV)

            b, g, r = cv2.split(img)
            self.filters[name] = cv2.merge([b, g, r, alpha])

    def get_landmarks(self, img, face):
        """ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        shape = self.predictor(img, face)
        landmarks = np.array([[shape.part(i).x, shape.part(i).y]
                              for i in range(68)])
        return landmarks

    def overlay_image(self, background, overlay, x, y):
        """íˆ¬ëª… ì´ë¯¸ì§€ í•©ì„±"""
        h, w = overlay.shape[:2]

        # ê²½ê³„ ì²´í¬
        if x < 0:
            overlay = overlay[:, -x:]
            w = overlay.shape[1]
            x = 0
        if y < 0:
            overlay = overlay[-y:, :]
            h = overlay.shape[0]
            y = 0

        bh, bw = background.shape[:2]
        if x + w > bw:
            overlay = overlay[:, :bw - x]
            w = overlay.shape[1]
        if y + h > bh:
            overlay = overlay[:bh - y, :]
            h = overlay.shape[0]

        if w <= 0 or h <= 0:
            return background

        # ì•ŒíŒŒ ë¸”ë Œë”©
        overlay_rgb = overlay[:, :, :3]
        alpha = overlay[:, :, 3] / 255.0

        roi = background[y:y+h, x:x+w]

        for c in range(3):
            roi[:, :, c] = (alpha * overlay_rgb[:, :, c] +
                           (1 - alpha) * roi[:, :, c])

        background[y:y+h, x:x+w] = roi

        return background

    def apply_sunglasses(self, img, landmarks, filter_img):
        """ì„ ê¸€ë¼ìŠ¤ í•„í„° ì ìš©"""
        # ëˆˆ ì¢Œí‘œ
        left_eye = landmarks[36:42].mean(axis=0).astype(int)
        right_eye = landmarks[42:48].mean(axis=0).astype(int)

        # ëˆˆ ì‚¬ì´ ê±°ë¦¬ì™€ ê°ë„
        eye_width = np.linalg.norm(right_eye - left_eye)
        eye_center = ((left_eye + right_eye) / 2).astype(int)
        angle = np.degrees(np.arctan2(right_eye[1] - left_eye[1],
                                      right_eye[0] - left_eye[0]))

        # ì„ ê¸€ë¼ìŠ¤ í¬ê¸° ì¡°ì •
        filter_width = int(eye_width * 2.5)
        filter_height = int(filter_width * filter_img.shape[0] /
                           filter_img.shape[1])

        resized_filter = cv2.resize(filter_img, (filter_width, filter_height))

        # íšŒì „
        M = cv2.getRotationMatrix2D((filter_width // 2, filter_height // 2),
                                    -angle, 1)
        rotated_filter = cv2.warpAffine(resized_filter, M,
                                        (filter_width, filter_height),
                                        flags=cv2.INTER_LINEAR,
                                        borderMode=cv2.BORDER_CONSTANT,
                                        borderValue=(0, 0, 0, 0))

        # ìœ„ì¹˜ ê³„ì‚°
        x = eye_center[0] - filter_width // 2
        y = eye_center[1] - filter_height // 2

        # í•©ì„±
        result = self.overlay_image(img, rotated_filter, x, y)

        return result

    def apply_hat(self, img, landmarks, filter_img):
        """ëª¨ì í•„í„° ì ìš©"""
        # ì´ë§ˆ ìœ„ì¹˜ (ëˆˆì¹ ìœ„)
        left_brow = landmarks[17:22].mean(axis=0)
        right_brow = landmarks[22:27].mean(axis=0)

        brow_center = ((left_brow + right_brow) / 2).astype(int)
        brow_width = np.linalg.norm(right_brow - left_brow)

        # ëª¨ì í¬ê¸°
        hat_width = int(brow_width * 3)
        hat_height = int(hat_width * filter_img.shape[0] /
                        filter_img.shape[1])

        resized_hat = cv2.resize(filter_img, (hat_width, hat_height))

        # ìœ„ì¹˜ (ëˆˆì¹ ìœ„ì— ë°°ì¹˜)
        x = brow_center[0] - hat_width // 2
        y = brow_center[1] - hat_height

        # í•©ì„±
        result = self.overlay_image(img, resized_hat, x, y)

        return result

    def apply_mustache(self, img, landmarks, filter_img):
        """ì½§ìˆ˜ì—¼ í•„í„° ì ìš©"""
        # ì½” ì•„ë˜, ì… ìœ„
        nose_tip = landmarks[33]
        upper_lip = landmarks[51]

        center = ((nose_tip + upper_lip) / 2).astype(int)

        # ì½§ìˆ˜ì—¼ í¬ê¸° (ì… ë„ˆë¹„ ê¸°ì¤€)
        mouth_width = np.linalg.norm(landmarks[48] - landmarks[54])
        mustache_width = int(mouth_width * 1.5)
        mustache_height = int(mustache_width * filter_img.shape[0] /
                             filter_img.shape[1])

        resized = cv2.resize(filter_img, (mustache_width, mustache_height))

        x = center[0] - mustache_width // 2
        y = center[1] - mustache_height // 2

        result = self.overlay_image(img, resized, x, y)

        return result

    def process(self, img, filter_name='sunglasses'):
        """í•„í„° ì ìš©"""
        if filter_name not in self.filters:
            return img

        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        faces = self.detector(rgb, 0)

        result = img.copy()

        for face in faces:
            landmarks = self.get_landmarks(rgb, face)

            if filter_name == 'sunglasses':
                result = self.apply_sunglasses(result, landmarks,
                                               self.filters[filter_name])
            elif filter_name == 'hat':
                result = self.apply_hat(result, landmarks,
                                        self.filters[filter_name])
            elif filter_name == 'mustache':
                result = self.apply_mustache(result, landmarks,
                                             self.filters[filter_name])

        return result

# ì‚¬ìš© ì˜ˆ
def realtime_face_filter():
    """ì‹¤ì‹œê°„ ì–¼êµ´ í•„í„°"""

    filter_app = FaceFilter('shape_predictor_68_face_landmarks.dat')

    # í•„í„° ë¡œë“œ (íˆ¬ëª… PNG ê¶Œì¥)
    filter_app.load_filter('sunglasses', 'sunglasses.png')
    # filter_app.load_filter('hat', 'hat.png')
    # filter_app.load_filter('mustache', 'mustache.png')

    cap = cv2.VideoCapture(0)

    current_filter = 'sunglasses'
    filters = list(filter_app.filters.keys())
    filter_idx = 0

    print("Press 'n' to change filter, 'q' to quit")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)

        # í•„í„° ì ìš©
        result = filter_app.process(frame, current_filter)

        # í˜„ì¬ í•„í„° í‘œì‹œ
        cv2.putText(result, f"Filter: {current_filter}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        cv2.imshow('Face Filter', result)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('n'):
            filter_idx = (filter_idx + 1) % len(filters)
            current_filter = filters[filter_idx]

    cap.release()
    cv2.destroyAllWindows()

# ì‹¤í–‰
# realtime_face_filter()
```

---

## í”„ë¡œì íŠ¸ 5: ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ ê°œìš”

```
ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ (Object Tracking System):
ë°°ê²½ ì°¨ë¶„ê³¼ ì¹¼ë§Œ í•„í„°ë¥¼ ì¡°í•©í•œ ë‹¤ì¤‘ ê°ì²´ ì¶”ì 

ì²˜ë¦¬ íë¦„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ í”„ë ˆì„  â”‚ â†’ â”‚ ë°°ê²½    â”‚ â†’ â”‚ ìœ¤ê³½ì„   â”‚ â†’ â”‚ ì¹¼ë§Œ    â”‚
â”‚ ì…ë ¥    â”‚    â”‚ ì°¨ë¶„    â”‚    â”‚ ê²€ì¶œ    â”‚    â”‚ í•„í„°    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê²°ê³¼    â”‚ â† â”‚ ID      â”‚ â† â”‚ í—ê°€ë¦¬ì•ˆâ”‚ â† â”‚ ì˜ˆì¸¡    â”‚
â”‚ ì¶œë ¥    â”‚    â”‚ í• ë‹¹    â”‚    â”‚ ë§¤ì¹­    â”‚    â”‚ ìœ„ì¹˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë‹¨ê³„ë³„ êµ¬í˜„

```python
import cv2
import numpy as np
from scipy.optimize import linear_sum_assignment

class KalmanTracker:
    """ì¹¼ë§Œ í•„í„° ê¸°ë°˜ ë‹¨ì¼ ê°ì²´ ì¶”ì ê¸°"""

    def __init__(self, initial_pos):
        # ì¹¼ë§Œ í•„í„° ì´ˆê¸°í™”
        # ìƒíƒœ ë²¡í„°: [x, y, vx, vy]
        self.kalman = cv2.KalmanFilter(4, 2)

        # ì „ì´ í–‰ë ¬ (ë“±ì† ìš´ë™ ëª¨ë¸)
        self.kalman.transitionMatrix = np.array([
            [1, 0, 1, 0],
            [0, 1, 0, 1],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ], dtype=np.float32)

        # ì¸¡ì • í–‰ë ¬
        self.kalman.measurementMatrix = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ], dtype=np.float32)

        # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        self.kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03

        # ì¸¡ì • ë…¸ì´ì¦ˆ
        self.kalman.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1

        # ì´ˆê¸° ìƒíƒœ
        self.kalman.statePre = np.array([
            [initial_pos[0]],
            [initial_pos[1]],
            [0],
            [0]
        ], dtype=np.float32)

        self.kalman.statePost = self.kalman.statePre.copy()

        self.age = 0  # ì¶”ì  í”„ë ˆì„ ìˆ˜
        self.hits = 1  # ì„±ê³µì ì¸ ë§¤ì¹­ ìˆ˜
        self.time_since_update = 0  # ì—…ë°ì´íŠ¸ ì´í›„ í”„ë ˆì„ ìˆ˜

    def predict(self):
        """ë‹¤ìŒ ìœ„ì¹˜ ì˜ˆì¸¡"""
        prediction = self.kalman.predict()
        self.age += 1
        self.time_since_update += 1
        return prediction[:2].flatten()

    def update(self, measurement):
        """ì¸¡ì •ê°’ìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        self.kalman.correct(np.array(measurement, dtype=np.float32))
        self.hits += 1
        self.time_since_update = 0

    def get_state(self):
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return self.kalman.statePost[:2].flatten()


class MultiObjectTracker:
    """ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ"""

    def __init__(self, max_age=30, min_hits=3, iou_threshold=0.3):
        self.trackers = []
        self.next_id = 0
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold

        # ë°°ê²½ ì°¨ë¶„ê¸°
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            history=500, varThreshold=16, detectShadows=True
        )

    def detect_objects(self, frame):
        """ë°°ê²½ ì°¨ë¶„ìœ¼ë¡œ ê°ì²´ ê²€ì¶œ"""
        # ë°°ê²½ ì°¨ë¶„
        fg_mask = self.bg_subtractor.apply(frame)

        # ê·¸ë¦¼ì ì œê±°
        fg_mask = cv2.threshold(fg_mask, 200, 255, cv2.THRESH_BINARY)[1]

        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)

        # ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL,
                                        cv2.CHAIN_APPROX_SIMPLE)

        detections = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 500:  # ìµœì†Œ ë©´ì 
                x, y, w, h = cv2.boundingRect(contour)
                center = (x + w // 2, y + h // 2)
                detections.append({
                    'bbox': (x, y, w, h),
                    'center': center
                })

        return detections, fg_mask

    def iou(self, bbox1, bbox2):
        """IoU (Intersection over Union) ê³„ì‚°"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2

        xi1 = max(x1, x2)
        yi1 = max(y1, y2)
        xi2 = min(x1 + w1, x2 + w2)
        yi2 = min(y1 + h1, y2 + h2)

        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)

        box1_area = w1 * h1
        box2_area = w2 * h2

        union_area = box1_area + box2_area - inter_area

        return inter_area / union_area if union_area > 0 else 0

    def associate_detections(self, detections):
        """ê²€ì¶œê³¼ íŠ¸ë˜ì»¤ ë§¤ì¹­ (í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜)"""
        if len(self.trackers) == 0:
            return [], list(range(len(detections))), []

        if len(detections) == 0:
            return [], [], list(range(len(self.trackers)))

        # ë¹„ìš© í–‰ë ¬ ê³„ì‚° (ê±°ë¦¬ ê¸°ë°˜)
        cost_matrix = np.zeros((len(detections), len(self.trackers)))

        for d, det in enumerate(detections):
            for t, tracker in enumerate(self.trackers):
                pred = tracker['kalman'].predict()
                dist = np.linalg.norm(np.array(det['center']) - pred)
                cost_matrix[d, t] = dist

        # í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì  ë§¤ì¹­
        row_indices, col_indices = linear_sum_assignment(cost_matrix)

        matched = []
        unmatched_detections = list(range(len(detections)))
        unmatched_trackers = list(range(len(self.trackers)))

        for row, col in zip(row_indices, col_indices):
            if cost_matrix[row, col] < 100:  # ê±°ë¦¬ ì„ê³„ê°’
                matched.append((row, col))
                unmatched_detections.remove(row)
                unmatched_trackers.remove(col)

        return matched, unmatched_detections, unmatched_trackers

    def update(self, frame):
        """ì¶”ì  ì—…ë°ì´íŠ¸"""
        # ê°ì²´ ê²€ì¶œ
        detections, fg_mask = self.detect_objects(frame)

        # ì˜ˆì¸¡
        for tracker in self.trackers:
            tracker['kalman'].predict()

        # ë§¤ì¹­
        matched, unmatched_dets, unmatched_trks = \
            self.associate_detections(detections)

        # ë§¤ì¹­ëœ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸
        for det_idx, trk_idx in matched:
            self.trackers[trk_idx]['kalman'].update(
                np.array(detections[det_idx]['center'])
            )
            self.trackers[trk_idx]['bbox'] = detections[det_idx]['bbox']

        # ìƒˆ íŠ¸ë˜ì»¤ ìƒì„±
        for det_idx in unmatched_dets:
            tracker = {
                'id': self.next_id,
                'kalman': KalmanTracker(detections[det_idx]['center']),
                'bbox': detections[det_idx]['bbox'],
                'color': (
                    np.random.randint(0, 255),
                    np.random.randint(0, 255),
                    np.random.randint(0, 255)
                )
            }
            self.trackers.append(tracker)
            self.next_id += 1

        # ì˜¤ë˜ëœ íŠ¸ë˜ì»¤ ì œê±°
        self.trackers = [t for t in self.trackers
                        if t['kalman'].time_since_update < self.max_age]

        # ê²°ê³¼ ë°˜í™˜
        results = []
        for tracker in self.trackers:
            if tracker['kalman'].hits >= self.min_hits:
                results.append({
                    'id': tracker['id'],
                    'bbox': tracker['bbox'],
                    'color': tracker['color'],
                    'center': tracker['kalman'].get_state()
                })

        return results, fg_mask

    def draw(self, frame, results):
        """ê²°ê³¼ ì‹œê°í™”"""
        for obj in results:
            x, y, w, h = obj['bbox']
            color = obj['color']

            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(frame, f"ID: {obj['id']}", (x, y-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # ê¶¤ì  í‘œì‹œ (ì¤‘ì‹¬ì )
            center = tuple(obj['center'].astype(int))
            cv2.circle(frame, center, 4, color, -1)

        return frame

# ì‚¬ìš© ì˜ˆ
def multi_object_tracking(video_path):
    """ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ì‹¤í–‰"""

    tracker = MultiObjectTracker()
    cap = cv2.VideoCapture(video_path)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # ì¶”ì  ì—…ë°ì´íŠ¸
        results, fg_mask = tracker.update(frame)

        # ì‹œê°í™”
        output = tracker.draw(frame, results)

        # ì •ë³´ í‘œì‹œ
        cv2.putText(output, f"Objects: {len(results)}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        cv2.imshow('Multi-Object Tracking', output)
        cv2.imshow('Foreground Mask', fg_mask)

        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# ì‹¤í–‰
# multi_object_tracking('traffic.mp4')
```

---

## ì—°ìŠµ ë¬¸ì œ ë° í™•ì¥ ì•„ì´ë””ì–´

### í”„ë¡œì íŠ¸ 1: ë¬¸ì„œ ìŠ¤ìºë„ˆ í™•ì¥

1. **OCR í†µí•©**: Tesseract OCRì„ ì—°ë™í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. **ìë™ ìƒ‰ìƒ ë³´ì •**: íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”ë¡œ ë¬¸ì„œ ê°€ë…ì„± í–¥ìƒ
3. **ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì›**: ì—°ì† ì´¬ì˜ìœ¼ë¡œ PDF ìƒì„±
4. **ì†ê¸€ì”¨ ì¸ì‹**: ì†ìœ¼ë¡œ ì“´ ë¬¸ì„œì˜ ë””ì§€í„¸í™”
5. **ì˜ìˆ˜ì¦ íŒŒì‹±**: ê¸ˆì•¡, ë‚ ì§œ ë“± ìë™ ì¶”ì¶œ

### í”„ë¡œì íŠ¸ 2: ì°¨ì„  ê²€ì¶œ í™•ì¥

1. **ê³¡ì„  ì°¨ì„  ê²€ì¶œ**: 2ì°¨/3ì°¨ ë‹¤í•­ì‹ í”¼íŒ…
2. **ì°¨ì„  ì´íƒˆ ê²½ê³ **: ì°¨ëŸ‰ ì¤‘ì‹¬ê³¼ ì°¨ì„  ì¤‘ì‹¬ ë¹„êµ
3. **ì•¼ê°„ ëª¨ë“œ**: ì¡°ëª… ì¡°ê±´ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì¡°ì •
4. **ë‹¤ì¤‘ ì°¨ì„  ê²€ì¶œ**: ì¸ì ‘ ì°¨ì„ ê¹Œì§€ ê²€ì¶œ
5. **ì°¨ëŸ‰ ê²€ì¶œ í†µí•©**: YOLOì™€ ê²°í•©í•˜ì—¬ ì•ì°¨ ê°ì§€

### í”„ë¡œì íŠ¸ 3: AR ë§ˆì»¤ í™•ì¥

1. **3D ëª¨ë¸ ë Œë”ë§**: OpenGLê³¼ ì—°ë™í•˜ì—¬ 3D ê°ì²´ í‘œì‹œ
2. **ë‹¤ì¤‘ ë§ˆì»¤ ì¸í„°ë™ì…˜**: ë§ˆì»¤ ê°„ ê´€ê³„ ì¸ì‹
3. **ë§ˆì»¤ ì—†ëŠ” AR**: í‰ë©´ ê²€ì¶œ ê¸°ë°˜ AR
4. **ê²Œì„ ê°œë°œ**: ë§ˆì»¤ ê¸°ë°˜ ê°„ë‹¨í•œ AR ê²Œì„
5. **ê°€êµ¬ ë°°ì¹˜ ì‹œë®¬ë ˆì´ì…˜**: ì‹¤ì œ ê³µê°„ì— ê°€ìƒ ê°€êµ¬ ë°°ì¹˜

### í”„ë¡œì íŠ¸ 4: ì–¼êµ´ í•„í„° í™•ì¥

1. **í‘œì • ì¸ì‹**: ì›ƒìŒ, ëˆˆ ê¹œë¹¡ì„ ê°ì§€í•˜ì—¬ í•„í„° ë³€ê²½
2. **3D í•„í„°**: ì–¼êµ´ í¬ì¦ˆì— ë§ì¶° 3D ë³€í˜•
3. **ë°°ê²½ êµì²´**: ì„¸ê·¸ë©˜í…Œì´ì…˜ìœ¼ë¡œ ë°°ê²½ë§Œ êµì²´
4. **ì–¼êµ´ ìŠ¤ì™‘**: ë‘ ì‚¬ëŒì˜ ì–¼êµ´ êµí™˜
5. **ì—ì´ì§• í•„í„°**: ì–¼êµ´ ë…¸í™”/ì Šì–´ì§€ê¸° íš¨ê³¼

### í”„ë¡œì íŠ¸ 5: ê°ì²´ ì¶”ì  í™•ì¥

1. **Re-ID ê¸°ëŠ¥**: í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°”ë‹¤ ë“¤ì–´ì˜¨ ê°ì²´ ì¬ì‹ë³„
2. **ì†ë„ ì¸¡ì •**: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í›„ ì‹¤ì œ ì†ë„ ê³„ì‚°
3. **ì˜ì—­ ì¹¨ì… ê°ì§€**: íŠ¹ì • ì˜ì—­ ì§„ì… ì‹œ ì•Œë¦¼
4. **ê¶¤ì  ë¶„ì„**: ì´ë™ íŒ¨í„´ ë¶„ì„ ë° ì´ìƒ ê°ì§€
5. **ë”¥ëŸ¬ë‹ í†µí•©**: YOLO + DeepSORTë¡œ ì •í™•ë„ í–¥ìƒ

---

## ë‹¤ìŒ ë‹¨ê³„

OpenCVì™€ ì»´í“¨í„° ë¹„ì „ì˜ ê¸°ì´ˆë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤. ë” ê¹Šì€ í•™ìŠµì„ ìœ„í•´ ë‹¤ìŒ ì£¼ì œë“¤ì„ ì¶”ì²œí•©ë‹ˆë‹¤:

### ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **PyTorch**: ì—°êµ¬ ë° í”„ë¡œí† íƒ€ì´í•‘ì— ê°•ë ¥
- **TensorFlow/Keras**: í”„ë¡œë•ì…˜ ë°°í¬ì— ì í•©
- **ONNX**: ëª¨ë¸ í˜¸í™˜ì„±ì„ ìœ„í•œ í‘œì¤€

### ê³ ê¸‰ ì»´í“¨í„° ë¹„ì „
- **ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜**: U-Net, Mask R-CNN
- **í¬ì¦ˆ ì¶”ì •**: OpenPose, MediaPipe
- **GAN ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±**: StyleGAN, Pix2Pix
- **3D ë¹„ì „**: ìŠ¤í…Œë ˆì˜¤ ë¹„ì „, ê¹Šì´ ì¶”ì •

### ì‘ìš© ë¶„ì•¼
- **ììœ¨ì£¼í–‰**: SLAM, ì„¼ì„œ í“¨ì „
- **ì˜ë£Œ ì˜ìƒ**: CT/MRI ë¶„ì„, ì§ˆë³‘ ê²€ì¶œ
- **ì‚°ì—… ê²€ì‚¬**: ê²°í•¨ ê²€ì¶œ, í’ˆì§ˆ ê´€ë¦¬
- **ë³´ì•ˆ/ê°ì‹œ**: ì´ìƒ í–‰ë™ ê°ì§€, ì–¼êµ´ ì¸ì‹

---

## ì°¸ê³  ìë£Œ

- [OpenCV Tutorials](https://docs.opencv.org/4.x/d9/df8/tutorial_root.html)
- [PyImageSearch](https://pyimagesearch.com/) - ì‹¤ì „ í”„ë¡œì íŠ¸ íŠœí† ë¦¬ì–¼
- [Learn OpenCV](https://learnopencv.com/) - ê³ ê¸‰ ì˜ˆì œ
- [Mediapipe](https://google.github.io/mediapipe/) - Googleì˜ ML ì†”ë£¨ì…˜
- [Papers With Code](https://paperswithcode.com/) - ìµœì‹  ì—°êµ¬ ë° ì½”ë“œ
- Bradski, G., & Kaehler, A. (2008). "Learning OpenCV"
- Szeliski, R. (2010). "Computer Vision: Algorithms and Applications"
