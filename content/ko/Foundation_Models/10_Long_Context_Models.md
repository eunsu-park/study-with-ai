# 10. Long Context Models

## ê°œìš”

í‘œì¤€ Transformerì˜ Self-Attentionì€ O(nÂ²) ë³µì¡ë„ë¡œ ì¸í•´ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë ˆìŠ¨ì—ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ í™•ì¥í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## 1. Context Lengthì˜ ì¤‘ìš”ì„±

### 1.1 ì™œ ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Long Context ì‚¬ìš© ì‚¬ë¡€                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ğŸ“š ë¬¸ì„œ ë¶„ì„                                                    â”‚
â”‚  - ë…¼ë¬¸ ì „ì²´ (10K-50K í† í°)                                     â”‚
â”‚  - ë²•ë¥  ë¬¸ì„œ (100K+ í† í°)                                       â”‚
â”‚  - ì±… ì „ì²´ ìš”ì•½                                                  â”‚
â”‚                                                                  â”‚
â”‚  ğŸ’» ì½”ë“œ ì´í•´                                                    â”‚
â”‚  - ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„                                          â”‚
â”‚  - ê¸´ í•¨ìˆ˜/í´ë˜ìŠ¤ ë¦¬íŒ©í† ë§                                       â”‚
â”‚  - ë©€í‹°íŒŒì¼ ë””ë²„ê¹…                                               â”‚
â”‚                                                                  â”‚
â”‚  ğŸ¤– Agent ì‹œìŠ¤í…œ                                                 â”‚
â”‚  - ê¸´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€                                         â”‚
â”‚  - ë³µì¡í•œ ë©€í‹°ìŠ¤í… íƒœìŠ¤í¬                                        â”‚
â”‚  - Tool ì‚¬ìš© ê¸°ë¡ ëˆ„ì                                            â”‚
â”‚                                                                  â”‚
â”‚  ğŸ” RAG ê°œì„                                                      â”‚
â”‚  - ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ í¬í•¨                                        â”‚
â”‚  - ë¬¸ì„œ ì¡°ê° ëŒ€ì‹  ì „ì²´ ë¬¸ì„œ ì œê³µ                                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ

| ëª¨ë¸ | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | ì¶œì‹œ ì‹œê¸° |
|------|---------------|-----------|
| GPT-3 | 2,048 | 2020 |
| GPT-3.5 | 4,096 / 16,384 | 2022-2023 |
| GPT-4 | 8,192 / 32,768 / 128K | 2023-2024 |
| Claude 2 | 100,000 | 2023 |
| Claude 3 | 200,000 | 2024 |
| Gemini 1.5 | 1,000,000 / 2,000,000 | 2024 |
| LLaMA 2 | 4,096 | 2023 |
| LLaMA 3 | 8,192 / 128K | 2024 |

---

## 2. íš¨ìœ¨ì ì¸ Attention ë©”ì»¤ë‹ˆì¦˜

### 2.1 Sparse Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Sparse Attention íŒ¨í„´                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Local Attention        Global Attention                   â”‚
â”‚  â–  â–  â–  â–¡ â–¡ â–¡ â–¡         â–  â–¡ â–¡ â–¡ â–¡ â–¡ â–¡                      â”‚
â”‚  â–  â–  â–  â–  â–¡ â–¡ â–¡         â–  â–  â–¡ â–¡ â–¡ â–¡ â–¡                      â”‚
â”‚  â–¡ â–  â–  â–  â–  â–¡ â–¡         â–  â–¡ â–  â–¡ â–¡ â–¡ â–¡                      â”‚
â”‚  â–¡ â–¡ â–  â–  â–  â–  â–¡         â–  â–¡ â–¡ â–  â–¡ â–¡ â–¡                      â”‚
â”‚  â–¡ â–¡ â–¡ â–  â–  â–  â–          â–  â–¡ â–¡ â–¡ â–  â–¡ â–¡                      â”‚
â”‚  â–¡ â–¡ â–¡ â–¡ â–  â–  â–          â–  â–¡ â–¡ â–¡ â–¡ â–  â–¡                      â”‚
â”‚  â–¡ â–¡ â–¡ â–¡ â–¡ â–  â–          â–  â–¡ â–¡ â–¡ â–¡ â–¡ â–                       â”‚
â”‚                                                             â”‚
â”‚  Longformer: Local + Global í† í° ì¡°í•©                       â”‚
â”‚  BigBird: Local + Global + Random                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Longformer êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class LongformerAttention(nn.Module):
    """
    Longformer: Sliding Window + Global Attention

    ë³µì¡ë„: O(n Ã— w) where w = window size
    """

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        window_size: int = 256,
        global_tokens: int = 2  # [CLS], [SEP]
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.window_size = window_size
        self.global_tokens = global_tokens

        # Q, K, V projections
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)

        # Global attentionìš© ë³„ë„ projection
        self.global_query = nn.Linear(hidden_size, hidden_size)
        self.global_key = nn.Linear(hidden_size, hidden_size)
        self.global_value = nn.Linear(hidden_size, hidden_size)

        self.output = nn.Linear(hidden_size, hidden_size)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Args:
            hidden_states: (batch, seq_len, hidden_size)
            attention_mask: (batch, seq_len)
        """
        batch_size, seq_len, _ = hidden_states.shape

        # Q, K, V ê³„ì‚°
        Q = self.query(hidden_states)
        K = self.key(hidden_states)
        V = self.value(hidden_states)

        # Reshape: (batch, seq_len, num_heads, head_dim)
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # 1. Sliding Window Attention (local)
        local_output = self._sliding_window_attention(Q, K, V)

        # 2. Global Attention (ì²˜ìŒ global_tokensê°œ)
        global_output = self._global_attention(
            hidden_states, Q, K, V
        )

        # ê²°í•© (global í† í° ìœ„ì¹˜ëŠ” global ê²°ê³¼ ì‚¬ìš©)
        output = local_output.clone()
        output[:, :self.global_tokens] = global_output[:, :self.global_tokens]

        # Output projection
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.output(output)

        return output

    def _sliding_window_attention(
        self,
        Q: torch.Tensor,
        K: torch.Tensor,
        V: torch.Tensor
    ) -> torch.Tensor:
        """
        Sliding Window Attention

        ê° í† í°ì€ window_size ë²”ìœ„ ë‚´ì˜ í† í°ë§Œ ì°¸ì¡°
        """
        batch_size, seq_len, num_heads, head_dim = Q.shape
        w = self.window_size // 2

        # íŒ¨ë”© ì¶”ê°€
        Q_padded = F.pad(Q, (0, 0, 0, 0, w, w), value=0)
        K_padded = F.pad(K, (0, 0, 0, 0, w, w), value=0)
        V_padded = F.pad(V, (0, 0, 0, 0, w, w), value=0)

        # ìœˆë„ìš° ì¶”ì¶œ (unfold)
        # ì‹¤ì œ êµ¬í˜„ì€ ë” ë³µì¡í•˜ì§€ë§Œ ê°œë… ì´í•´ìš© ê°„ì†Œí™” ë²„ì „
        output = torch.zeros_like(Q)

        for i in range(seq_len):
            # ië²ˆì§¸ í† í°ì˜ ìœˆë„ìš°: [i, i + window_size]
            start = i
            end = i + self.window_size

            q_i = Q[:, i:i+1]  # (batch, 1, heads, dim)
            k_window = K_padded[:, start:end]  # (batch, window, heads, dim)
            v_window = V_padded[:, start:end]

            # Attention
            scores = torch.einsum('bihd,bjhd->bijh', q_i, k_window)
            scores = scores / math.sqrt(head_dim)
            weights = F.softmax(scores, dim=2)
            out_i = torch.einsum('bijh,bjhd->bihd', weights, v_window)

            output[:, i] = out_i[:, 0]

        return output

    def _global_attention(
        self,
        hidden_states: torch.Tensor,
        Q: torch.Tensor,
        K: torch.Tensor,
        V: torch.Tensor
    ) -> torch.Tensor:
        """Global Attention: global í† í°ì€ ì „ì²´ ì‹œí€€ìŠ¤ ì°¸ì¡°"""
        batch_size, seq_len, _ = hidden_states.shape

        # Global í† í°ë§Œ ì¶”ì¶œ
        global_hidden = hidden_states[:, :self.global_tokens]

        # Global Q, K, V
        global_Q = self.global_query(global_hidden)
        global_K = self.global_key(hidden_states)
        global_V = self.global_value(hidden_states)

        # ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ attention
        global_Q = global_Q.view(batch_size, self.global_tokens,
                                  self.num_heads, self.head_dim)
        global_K = global_K.view(batch_size, seq_len,
                                  self.num_heads, self.head_dim)
        global_V = global_V.view(batch_size, seq_len,
                                  self.num_heads, self.head_dim)

        # (batch, global, heads, seq) attention
        scores = torch.einsum('bghd,bshd->bghs', global_Q, global_K)
        scores = scores / math.sqrt(self.head_dim)
        weights = F.softmax(scores, dim=-1)

        # Output: (batch, global, heads, dim)
        output = torch.einsum('bghs,bshd->bghd', weights, global_V)

        return output
```

### 2.3 Flash Attention

```python
# Flash Attentionì€ CUDA ì»¤ë„ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŒ
# ì—¬ê¸°ì„œëŠ” ê°œë…ë§Œ ì„¤ëª…

"""
Flash Attention í•µì‹¬ ì•„ì´ë””ì–´:

1. íƒ€ì¼ë§ (Tiling):
   - Q, K, Vë¥¼ SRAMì— ë§ëŠ” ë¸”ë¡ìœ¼ë¡œ ë¶„í• 
   - HBM â†” SRAM ë°ì´í„° ì „ì†¡ ìµœì†Œí™”

2. ì¬ê³„ì‚° (Recomputation):
   - Forwardì—ì„œ attention weights ì €ì¥ ì•ˆ í•¨
   - Backwardì—ì„œ í•„ìš”í•  ë•Œ ì¬ê³„ì‚°
   - ë©”ëª¨ë¦¬ ì ˆì•½ (O(n) vs O(nÂ²))

3. ê²°ê³¼:
   - ë©”ëª¨ë¦¬: O(n) vs O(nÂ²)
   - ì†ë„: 2-4x ë¹ ë¦„
   - ì •í™•ë„: ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼
"""

# PyTorch 2.0+ì—ì„œ ì‚¬ìš©
def use_flash_attention():
    import torch.nn.functional as F

    # Scaled Dot-Product Attention (Flash Attention ìë™ ì‚¬ìš©)
    Q = torch.randn(2, 8, 1024, 64, device='cuda')
    K = torch.randn(2, 8, 1024, 64, device='cuda')
    V = torch.randn(2, 8, 1024, 64, device='cuda')

    # PyTorch 2.0+ SDPA
    with torch.backends.cuda.sdp_kernel(
        enable_flash=True,
        enable_math=False,
        enable_mem_efficient=False
    ):
        output = F.scaled_dot_product_attention(Q, K, V)

    return output


# xFormers ì‚¬ìš©
def use_xformers():
    from xformers.ops import memory_efficient_attention

    Q = torch.randn(2, 1024, 8, 64, device='cuda')
    K = torch.randn(2, 1024, 8, 64, device='cuda')
    V = torch.randn(2, 1024, 8, 64, device='cuda')

    output = memory_efficient_attention(Q, K, V)
    return output
```

---

## 3. ìœ„ì¹˜ ì¸ì½”ë”© í™•ì¥

### 3.1 ë¬¸ì œ: í•™ìŠµ ê¸¸ì´ë¥¼ ë„˜ì–´ì„œ ì™¸ì‚½

```
í•™ìŠµ: 4096 í† í°
ì¶”ë¡ : 8192+ í† í°

ë¬¸ì œ:
- ì ˆëŒ€ ìœ„ì¹˜ ì¸ì½”ë”©: 4096 ì´í›„ ìœ„ì¹˜ í•™ìŠµ ì•ˆ ë¨
- RoPE: ë³´ê°„/ì™¸ì‚½ í•„ìš”
```

### 3.2 Position Interpolation (PI)

```python
def linear_position_interpolation(
    position_ids: torch.Tensor,
    original_max_length: int,
    extended_max_length: int
) -> torch.Tensor:
    """
    Linear Position Interpolation

    ì•„ì´ë””ì–´: ìƒˆ ìœ„ì¹˜ë¥¼ ì›ë³¸ ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§

    position_idsë¥¼ [0, original_max_length)ë¡œ ì••ì¶•
    """
    scale = original_max_length / extended_max_length
    return position_ids.float() * scale


class RoPEWithInterpolation(nn.Module):
    """Position Interpolationì´ ì ìš©ëœ RoPE"""

    def __init__(
        self,
        dim: int,
        original_max_length: int = 4096,
        extended_max_length: int = 16384,
        base: float = 10000.0
    ):
        super().__init__()
        self.dim = dim
        self.original_max_length = original_max_length
        self.extended_max_length = extended_max_length
        self.base = base

        # ì£¼íŒŒìˆ˜ ê³„ì‚°
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)

        # ìŠ¤ì¼€ì¼ íŒ©í„°
        self.scale = original_max_length / extended_max_length

    def forward(
        self,
        x: torch.Tensor,
        position_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, heads, dim)
            position_ids: (batch, seq_len)
        """
        # ìœ„ì¹˜ ë³´ê°„
        scaled_positions = position_ids.float() * self.scale

        # ì£¼íŒŒìˆ˜ ê³„ì‚°
        freqs = torch.einsum('bi,d->bid', scaled_positions, self.inv_freq)
        emb = torch.cat([freqs, freqs], dim=-1)

        cos = emb.cos().unsqueeze(2)  # (batch, seq, 1, dim)
        sin = emb.sin().unsqueeze(2)

        # RoPE ì ìš©
        x_rope = self._apply_rope(x, cos, sin)

        return x_rope

    def _apply_rope(self, x, cos, sin):
        """RoPE ì ìš©"""
        x1 = x[..., : self.dim // 2]
        x2 = x[..., self.dim // 2 :]

        rotated = torch.cat([-x2, x1], dim=-1)
        return x * cos + rotated * sin
```

### 3.3 YaRN (Yet another RoPE extension method)

```python
class YaRNRoPE(nn.Module):
    """
    YaRN: NTK-aware Interpolation

    Position Interpolationì˜ ë¬¸ì œ:
    - ê³ ì£¼íŒŒ ì •ë³´ ì†ì‹¤ (ë†’ì€ ì°¨ì›)

    YaRN í•´ê²°ì±…:
    - ì €ì£¼íŒŒ: ë³´ê°„ (interpolation)
    - ê³ ì£¼íŒŒ: ì™¸ì‚½ (extrapolation)
    - NTK ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì£¼íŒŒìˆ˜ ì¡°ì •
    """

    def __init__(
        self,
        dim: int,
        original_max_length: int = 4096,
        extended_max_length: int = 32768,
        base: float = 10000.0,
        beta_fast: float = 32,
        beta_slow: float = 1,
    ):
        super().__init__()
        self.dim = dim
        self.original_max_length = original_max_length
        self.extended_max_length = extended_max_length

        scale = extended_max_length / original_max_length

        # ì°¨ì›ë³„ ë³´ê°„ ë¹„ìœ¨ ê³„ì‚°
        # ì €ì£¼íŒŒ (ë‚®ì€ ì°¨ì›): ë” ë§ì´ ë³´ê°„
        # ê³ ì£¼íŒŒ (ë†’ì€ ì°¨ì›): ëœ ë³´ê°„ (ì™¸ì‚½ì— ê°€ê¹Œì›€)
        dims = torch.arange(0, dim, 2)
        low = max(0, math.floor(dim * math.log(scale) / (2 * math.log(original_max_length))))
        high = min(dim // 2 - 1, math.ceil(dim * math.log(scale) / (2 * math.log(original_max_length))))

        # ë¨í”„ í•¨ìˆ˜ë¡œ ë³´ê°„/ì™¸ì‚½ ë¹„ìœ¨ ê²°ì •
        ramp = torch.zeros(dim // 2)
        ramp[:low] = 0.0  # ì™¸ì‚½
        ramp[high:] = 1.0  # ë³´ê°„

        if high > low:
            ramp[low:high] = (dims[low:high] - low) / (high - low)

        # NTK-aware base ì¡°ì •
        inv_freq = 1.0 / (base ** (dims.float() / dim))

        # ë³´ê°„ê³¼ ì™¸ì‚½ì˜ í˜¼í•©
        inv_freq_inter = inv_freq / scale
        self.register_buffer(
            'inv_freq',
            (1 - ramp) * inv_freq + ramp * inv_freq_inter
        )

        # Attention scaling
        self.mscale = 0.1 * math.log(scale) + 1.0

    def forward(self, x: torch.Tensor, position_ids: torch.Tensor):
        # ì£¼íŒŒìˆ˜ ê³„ì‚° (ì´ë¯¸ ì¡°ì •ëœ inv_freq ì‚¬ìš©)
        freqs = torch.einsum('bi,d->bid', position_ids.float(), self.inv_freq)
        emb = torch.cat([freqs, freqs], dim=-1)

        cos = emb.cos().unsqueeze(2) * self.mscale
        sin = emb.sin().unsqueeze(2) * self.mscale

        return self._apply_rope(x, cos, sin)
```

---

## 4. ALiBi (Attention with Linear Biases)

### 4.1 ê°œë…

```
ALiBi: í•™ìŠµ ì—†ëŠ” ìœ„ì¹˜ ì¸ì½”ë”©

ì•„ì´ë””ì–´:
- ìœ„ì¹˜ ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- ëŒ€ì‹ , attention ì ìˆ˜ì— ê±°ë¦¬ ê¸°ë°˜ íŒ¨ë„í‹° ì¶”ê°€
- ë©€ë¦¬ ìˆëŠ” í† í°ì¼ìˆ˜ë¡ attention ì ìˆ˜ ê°ì†Œ

Attention score modification:
score(q_i, k_j) = q_i Â· k_j - m Ã— |i - j|

m: headë³„ ê¸°ìš¸ê¸° (ê³ ì •, í•™ìŠµ ì•ˆ í•¨)
m_h = 2^(-8/H) for head h (H = ì´ head ìˆ˜)
```

### 4.2 êµ¬í˜„

```python
class ALiBiAttention(nn.Module):
    """ALiBi: Attention with Linear Biases"""

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        max_seq_len: int = 8192
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads

        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size, hidden_size)

        # ALiBi slopes: ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ
        # 2^(-8/n), 2^(-8*2/n), ..., 2^(-8)
        slopes = self._get_alibi_slopes(num_heads)
        self.register_buffer('slopes', slopes)

        # ê±°ë¦¬ í–‰ë ¬ ì‚¬ì „ ê³„ì‚°
        positions = torch.arange(max_seq_len)
        distance_matrix = positions.unsqueeze(0) - positions.unsqueeze(1)
        distance_matrix = distance_matrix.abs()
        self.register_buffer('distance_matrix', distance_matrix)

    def _get_alibi_slopes(self, num_heads: int) -> torch.Tensor:
        """Headë³„ ALiBi slope ê³„ì‚°"""

        def get_slopes_power_of_2(n):
            start = 2 ** (-(2 ** -(math.log2(n) - 3)))
            ratio = start
            return [start * ratio ** i for i in range(n)]

        if math.log2(num_heads).is_integer():
            slopes = get_slopes_power_of_2(num_heads)
        else:
            # ê°€ì¥ ê°€ê¹Œìš´ 2ì˜ ê±°ë“­ì œê³±ìœ¼ë¡œ ë³´ê°„
            closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))
            slopes = get_slopes_power_of_2(closest_power_of_2)

            extra_slopes = get_slopes_power_of_2(2 * closest_power_of_2)
            extra_slopes = extra_slopes[0::2][:num_heads - closest_power_of_2]
            slopes = slopes + extra_slopes

        return torch.tensor(slopes).view(1, num_heads, 1, 1)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor = None
    ) -> torch.Tensor:
        batch_size, seq_len, _ = hidden_states.shape

        # Q, K, V ê³„ì‚°
        Q = self.query(hidden_states)
        K = self.key(hidden_states)
        V = self.value(hidden_states)

        # Reshape
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose: (batch, heads, seq, dim)
        Q = Q.transpose(1, 2)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # ALiBi bias: -m Ã— |i - j|
        alibi_bias = -self.slopes * self.distance_matrix[:seq_len, :seq_len]
        scores = scores + alibi_bias

        # Causal mask
        causal_mask = torch.triu(
            torch.ones(seq_len, seq_len, device=scores.device) * float('-inf'),
            diagonal=1
        )
        scores = scores + causal_mask

        # Attention weights
        weights = F.softmax(scores, dim=-1)

        # Output
        output = torch.matmul(weights, V)
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.output(output)

        return output
```

---

## 5. Ring Attention

### 5.1 ê°œë…

```
Ring Attention: ë¶„ì‚° Long Context

ì•„ì´ë””ì–´:
- ì‹œí€€ìŠ¤ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°
- ê° GPUê°€ ìì‹ ì˜ ì²­í¬ + ìˆœí™˜í•˜ëŠ” KV ì²˜ë¦¬
- í†µì‹ ê³¼ ê³„ì‚° ì˜¤ë²„ë©

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Ring Attention                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  GPU 0: Q[0:n/4]     GPU 1: Q[n/4:n/2]        â”‚
â”‚          â†“ KV ìˆœí™˜        â†“ KV ìˆœí™˜            â”‚
â”‚  Step 1: K[0:n/4]    Step 1: K[n/4:n/2]       â”‚
â”‚  Step 2: K[n/4:n/2]  Step 2: K[n/2:3n/4]      â”‚
â”‚  Step 3: K[n/2:3n/4] Step 3: K[3n/4:n]        â”‚
â”‚  Step 4: K[3n/4:n]   Step 4: K[0:n/4]         â”‚
â”‚                                                â”‚
â”‚  KVê°€ ë§ì²˜ëŸ¼ ìˆœí™˜í•˜ë©° ê° GPUì˜ Qì™€ ê²°í•©         â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 êµ¬í˜„ ê°œìš”

```python
import torch.distributed as dist

def ring_attention_forward(
    Q: torch.Tensor,  # Local Q chunk
    K: torch.Tensor,  # Local K chunk
    V: torch.Tensor,  # Local V chunk
    world_size: int,
    rank: int
):
    """
    Ring Attention Forward Pass (ê°œë…ì  êµ¬í˜„)

    ì‹¤ì œ êµ¬í˜„ì€ CUDA ì»¤ë„ê³¼ ë³µì¡í•œ ë™ê¸°í™” í•„ìš”
    """
    local_seq_len = Q.shape[1]

    # ëˆ„ì  attention ì¶œë ¥
    output = torch.zeros_like(Q)
    max_scores = torch.full((Q.shape[0], Q.shape[2], local_seq_len), float('-inf'))
    sum_exp = torch.zeros_like(max_scores)

    # í˜„ì¬ KV
    current_K = K.clone()
    current_V = V.clone()

    for step in range(world_size):
        # ì´ ì²­í¬ì˜ KVì— ëŒ€í•´ attention ê³„ì‚°
        scores = torch.matmul(Q, current_K.transpose(-2, -1))
        scores = scores / math.sqrt(Q.shape[-1])

        # Online softmax (numerically stable)
        new_max = torch.max(scores.max(dim=-1).values, max_scores)
        exp_scores = torch.exp(scores - new_max.unsqueeze(-1))

        # ì´ì „ ê²°ê³¼ ìŠ¤ì¼€ì¼ ì¡°ì •
        scale = torch.exp(max_scores - new_max)
        output = output * scale.unsqueeze(-1) + torch.matmul(exp_scores, current_V)

        sum_exp = sum_exp * scale + exp_scores.sum(dim=-1)
        max_scores = new_max

        # KVë¥¼ ë‹¤ìŒ GPUë¡œ ì „ì†¡ (ring)
        if step < world_size - 1:
            # ë¹„ë™ê¸° send/recv
            send_rank = (rank + 1) % world_size
            recv_rank = (rank - 1) % world_size

            # ë‹¤ìŒ GPUì—ì„œ KV ìˆ˜ì‹ 
            current_K = ring_pass(current_K, send_rank, recv_rank)
            current_V = ring_pass(current_V, send_rank, recv_rank)

    # ìµœì¢… ì •ê·œí™”
    output = output / sum_exp.unsqueeze(-1)

    return output


def ring_pass(tensor, send_rank, recv_rank):
    """Ring topologyì—ì„œ í…ì„œ ì „ë‹¬"""
    recv_tensor = torch.empty_like(tensor)

    send_op = dist.isend(tensor, send_rank)
    recv_op = dist.irecv(recv_tensor, recv_rank)

    send_op.wait()
    recv_op.wait()

    return recv_tensor
```

---

## 6. ì‹¤ìš©ì  ê°€ì´ë“œ

### 6.1 ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë°©ë²• ì„ íƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ?                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  4K â†’ 8K:                                                    â”‚
â”‚  - Position Interpolation (ê°„ë‹¨, ì„±ëŠ¥ ì¢‹ìŒ)                  â”‚
â”‚  - ì•½ê°„ì˜ fine-tuning ê¶Œì¥                                   â”‚
â”‚                                                              â”‚
â”‚  4K â†’ 32K:                                                   â”‚
â”‚  - YaRN (PIë³´ë‹¤ ì„±ëŠ¥ ì¢‹ìŒ)                                   â”‚
â”‚  - ë˜ëŠ” ALiBi (ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œ)                             â”‚
â”‚                                                              â”‚
â”‚  32K â†’ 100K+:                                                â”‚
â”‚  - Flash Attention í•„ìˆ˜                                      â”‚
â”‚  - Ring Attention (ë‹¤ì¤‘ GPU)                                 â”‚
â”‚  - Sparse Attention ê³ ë ¤                                     â”‚
â”‚                                                              â”‚
â”‚  1M+:                                                        â”‚
â”‚  - íŠ¹ìˆ˜ ì•„í‚¤í…ì²˜ í•„ìš”                                        â”‚
â”‚  - Mamba/State Space Models                                  â”‚
â”‚  - ë˜ëŠ” ê·¹ë„ë¡œ í¬ì†Œí•œ attention                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 ì‹¤ì „ íŒ

```python
# 1. Gradient Checkpointingì€ í•„ìˆ˜
model.gradient_checkpointing_enable()

# 2. Mixed Precision ì‚¬ìš©
with torch.cuda.amp.autocast(dtype=torch.bfloat16):
    outputs = model(**inputs)

# 3. KV Cache ìµœì í™” (ì¶”ë¡  ì‹œ)
# - Sliding Window Cache
# - Paged Attention (vLLM)

# 4. ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ (ê¸´ ë¬¸ì„œ)
def process_long_document(model, document, chunk_size=4096, overlap=512):
    """ê¸´ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ  ì²˜ë¦¬"""
    tokens = tokenizer.encode(document)
    results = []

    for i in range(0, len(tokens), chunk_size - overlap):
        chunk = tokens[i:i + chunk_size]
        output = model.generate(chunk)
        results.append(output)

    return merge_results(results)
```

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- Beltagy et al. (2020). "Longformer: The Long-Document Transformer"
- Dao et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention"
- Press et al. (2021). "Train Short, Test Long: Attention with Linear Biases"
- Peng et al. (2023). "YaRN: Efficient Context Window Extension of Large Language Models"

### ê´€ë ¨ ë ˆìŠ¨
- [08_LLaMA_Family.md](08_LLaMA_Family.md) - RoPE ê¸°ë³¸
- [09_Mistral_MoE.md](09_Mistral_MoE.md) - Sliding Window Attention
